# Project Proposal: 

- Assign separete agents for each collection
- create vector DB for proposals
- better agents (give examples)
- add deep searcher (look at https://github.com/blackadad/paper-scraper/tree/main, https://github.com/Future-House/paper-qa/blob/main/docs/tutorials/where_do_I_get_papers.md )


## May Be
- use BERTopic or LLM (ask to generate yes/no questions based on abstracts) to get sparse vector for each paper pk => zk (BERTopic may be supports hierarchical topics)
- parse htmls to find important citations, build impact graph p1,..pk => pN 
- test if one can build regression z1+...zk => zN (hyperdimentional model ??), 
- Question:how do we know which papers to take to build new one, may be from literature review? but do we need regression and so on may be llm can generate proposal?




1. Assign Separate Agents for Each Collection
This is a powerful concept. We can think of these as "Collection Expert Agents."
Concept: Instead of a single, general PaperQA agent that selects a collection to query, imagine if each collection had its own dedicated agent instance. When a user creates a new collection of papers on "Quantum Machine Learning," a specialized QML agent is born alongside it.
User Benefit: This would create hyper-specialized experts. When you chat with the "Quantum Machine Learning" agent, its context is permanently locked to that collection, likely leading to more nuanced and accurate answers. It also allows for better mental modeling for the userâ€”they aren't just talking to a generic AI, they are talking to their expert on a specific topic.
Implementation Sketch:
We would need a mechanism to manage these agents, perhaps a new tab or a section in the "Collections" UI.
When the user selects a collection to chat with, the backend would load the specific agent configuration associated with it.

2. Create Vector DB for Proposals
This is a fantastic idea for long-term knowledge management. Let's call it a "Proposal Archive & Discovery System."
Concept: Every research proposal generated by the agent teams is not just a final artifact but is also chunked, vectorized, and stored in a dedicated vector database.
User Benefit:
Semantic Search: You could search your past proposals not by keyword, but by concept. "Find my previous proposals related to using GANs for data augmentation."
Automated Novelty Checks: Before embarking on generating a new proposal, an agent could first check this database to see if you've already explored a very similar idea, preventing redundant work.
Insight & Analytics: You could perform meta-analysis on your own work. "What research themes have I been most focused on this year?"
Implementation Sketch:
We could use a persistent vector store like ChromaDB.
A new step would be added to the end of the proposal generation graph: "Archive Final Proposal."

3. Better Agents (with examples from real reviews on OpenReview)
This is about increasing the diversity and capability of the agent teams. Here are a few concrete agent roles we could introduce:
The "Skeptic" Agent:
Role: A deliberately pessimistic agent whose only job is to find flaws. It would be prompted to challenge the methodology, question the feasibility, and poke holes in the proposal's claims of novelty.
Benefit: Introduces a valuable "red team" dynamic, forcing the proposal to be more robust.
The "Figure Generator" Agent:
Role: Reads the methodology and results sections of a proposal and suggests figures, diagrams, or charts.
Benefit: Helps in visualizing complex ideas and makes the final proposal more impactful. It could even be given a tool to generate Mermaid.js syntax for flowcharts.
The "Ethical Reviewer" Agent:
Role: Specifically for research in sensitive domains (like AI, genetics, etc.), this agent would review the proposal for potential ethical blind spots or concerns.
Benefit: Promotes responsible and thoughtful research design.

4. Add Deep Searcher
Based on the links you provided, this points to an "Automated Literature Discovery" feature, which is a game-changer.
Concept: The agent workflow is no longer limited to the papers you have manually added to your collections. A new agent, the "Deep Searcher," would be responsible for actively finding and fetching new literature from the web.
User Benefit: This automates one of the most time-consuming parts of research. The system can now build its own up-to-the-minute, relevant collection of papers for any given research question, ensuring the literature review is comprehensive and current.
Implementation Sketch (using the libraries you suggested):
The Query Generation Team creates a set of search terms.
The Deep Searcher agent takes these terms and uses a tool built on top of paper-scraper or a similar library.
This tool queries online sources like ArXiv, Semantic Scholar, etc.
It downloads the top N most relevant papers and automatically creates a new, temporary collection for them.
The Literature Review Team then proceeds, using this freshly curated, highly relevant collection.
These are all exciting directions. The Deep Searcher seems like a very concrete and high-impact feature to start with, especially since you've already identified some promising libraries to build upon.