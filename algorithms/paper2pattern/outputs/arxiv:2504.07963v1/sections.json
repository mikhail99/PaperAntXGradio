[
  {
    "title": "Table of Contents",
    "level": 2,
    "content": "Abstract 1 Introduction 2 Related Work Latent Space Diffusion/Flow Models. Pixel Space Diffusion/Flow Models. 3 PixelFlow 3.1 Preliminary: Flow Matching 3.2 Multi-Scale Generation in Pixel Space 3.3 Model Architecture Patchify. RoPE. Resolution Embedding. Text-to-Image Generation. 3.4 Training and Inference 4 Experiments 4.1 Experimental Setup 4.2 Model Design Kickoff sequence length. Patch size. 4.3 Inference Schedule Number of sample steps. ODE Solver. CFG Schedule. 4.4 Comparison on ImageNet Benchmark 4.5 Text-to-Image Generation Settings. Quantitative results. Visualization. 5 Conclusion Limitations References"
  },
  {
    "title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "level": 1,
    "content": "Shoufa Chen1 Chongjian Ge1,2 Shilong Zhang1 Peize Sun1 Ping Luo1 1The University of Hong Kong 2Adobe"
  },
  {
    "title": "Abstract",
    "level": 6,
    "content": "We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256√ó\\times√ó256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow."
  },
  {
    "title": "1 Introduction",
    "level": 2,
    "content": "Numquam ponenda est pluralitas sine necessitate. ‚Äî William of Ockham Figure 1: Comparisons of Design Paradigms between latent-based diffusion models (LDMs), pixel-based diffusion models (PDMs), and PixelFlow: (a) LDMs split training into two separate stages‚Äîfirst independently training off-the-shell VAEs, then training diffusion models on tokens extracted from the pre-trained VAEs; (b) Previous PDMs typically train two separate models: a diffusion model on low-resolution images and an upsampler for high-resolution synthesis; (c) PixelFlow, by contrast, offers an end-to-end solution for pixel-based generation, combining both high efficiency and strong generative performance. Driven by the success of the Stable Diffusion (SD) model series [50, 47, 46, 17], latent diffusion models (LDMs) [50] have emerged as the de facto standard for generative modeling across diverse modalities, spanning image [17, 45, 35], video [23, 66, 7, 8, 69], audio [39, 18], and 3D [57, 67]. As shown in Figure 1 (a), LDMs compress raw data into a compact latent space using pre-trained Variational Autoencoders (VAEs). This compression reduces computational demands and facilitates efficient diffusion denoising. Despite their widespread success, LDMs decouple the VAE and diffusion components, hindering joint optimization and complicating holistic diagnosis. An alternative approach is to implement diffusion models in the raw pixel space. While intuitive, this becomes computationally unaffordable for high-resolution images due to the substantial resources required to process per-pixel correlations. Considering this, prior research [52, 20, 22, 51, 44] has typically adopted a cascaded approach: first generating a low-resolution image, then employing additional upsamplers to produce high-quality outputs, with the low-resolution image serving as conditioning input, as shown in Figure 1(b). However, these cascaded methods also introduce separate networks for different stages, still limiting the benefits of end-to-end design. In this work, we introduce PixelFlow, a simple but effective end-to-end framework for direct image generation in raw pixel space, without the need of separate networks like VAEs or upsamplers. As illustrated in Figure 1(c), PixelFlow uses a unified set of parameters to model multi-scale samples across cascading resolutions via Flow Matching [40, 38]. At early denoising stages, when noise levels are high, PixelFlow operates on lower-resolution samples. As denoising progresses, the resolution gradually increases until it reaches the target resolution in the final stage. This progressive strategy avoids performing all denoising steps at full resolution, thereby significantly reducing the overall computational cost of the generation process. During training, the cross-scale samples at different timesteps are constructed by: (1) resizing the images to successive scales and adding Gaussian noise to each scaled image; (2) interpolating between adjacent scale noisy images as model input and conducting velocity prediction. The entire model is trained end-to-end using uniformly sampled training examples from all stages. During inference, the process begins with pure Gaussian noise at the lowest resolution. The model then progressively denoises and upscales the image until the target resolution is reached. We evaluated PixelFlow on both class-conditional and text-to-image generation tasks. Compared to established latent-space diffusion models [50, 45, 42], PixelFlow delivers competitive performance. For instance, on the 256√ó256256256256\\times 256256 √ó 256 ImageNet class-conditional generation benchmark, PixelFlow achieves an FID of 1.98. For text-to-image generation, PixelFlow is evaluated on widely-used benchmarks, achieving 0.64 on GenEval [19] and 77.93 on DPG-Bench [26]. In addition, qualitative results in Figure 5 and Figure 6 illustrate that PixelFlow has strong visual fidelity and text-image alignment, highlighting the potential of pixel-space generation for future research. The contributions of PixelFlow are summarized as in the following three points: ‚Ä¢ By eliminating the need for a pre-trained VAE, we establish an end-to-end trainable image generation model in raw pixel space directly. ‚Ä¢ Through cascade flow modeling from low resolution to high resolution, our model achieves affordable computation cost in both training and inference. ‚Ä¢ PixelFlow obtains competitive performance in visual quality, including 1.98 FID on 256√ó256256256256\\times 256256 √ó 256 ImageNet class-conditional image generation benchmark and appealing properties on text-to-image generation."
  },
  {
    "title": "Latent Space Diffusion/Flow Models.",
    "level": 4,
    "content": "Variational Autoencoders (VAEs) have become a core component in many recent generative models [16, 48, 50, 47, 59, 17, 66, 35], enabling the mapping of visual data from pixel space to a lower-dimensional, perceptually equivalent latent space. This compact representation facilitates more efficient training and inference. However, VAEs often compromise high-frequency details [47], leading to inevitable low-level artifacts in generated outputs. Motivated by a desire for algorithmic simplicity and fully end-to-end optimization, we forgo the VAE and operate directly in pixel space."
  },
  {
    "title": "Pixel Space Diffusion/Flow Models.",
    "level": 4,
    "content": "Early diffusion models [56, 21, 2] primarily operated directly in pixel space, aiming to capture the distributions images in a single stage. However, this approach proved both challenging and inefficient for high-resolution image generation, leading to the development of cascaded models [52, 20, 22, 30] that generate images through a sequence of stages. These cascaded models typically begin with the generation of a low-resolution image, which is subsequently upscaled by super-resolution models to achieve higher resolutions. However, the diffusion-based super-resolution process often requires starting from pure noise, conditioned on lower-resolution outputs, resulting in a time-consuming and inefficient generation process. Additionally, training these models in isolated stages hinders end-to-end optimization and necessitates carefully designed strategies to ensure the super-resolution stages. Furthermore, recent advancements in pixel-space generation have introduced innovative architectures. Simple Diffusion [24, 25] proposes a streamlined diffusion framework for high-resolution image synthesis, achieving strong performance on ImageNet through adjustments of model architecture and noise schedules. FractalGen [37] constructs fractal generative models by recursively invoking atomic generative modules, resulting in self-similar architectures that demonstrate strong performance in pixel-by-pixel image generation. TarFlow [68] presents a Transformer-based normalizing flow architecture capable of directly modeling and generating pixels."
  },
  {
    "title": "3.1 Preliminary: Flow Matching",
    "level": 3,
    "content": "The Flow Matching algorithm [1, 38, 40] progressively transforms a sample from a prior distribution, which is typically a standard normal distribution, to the target data distribution. This is accomplished by defining a forward process consisting of a sequence of linear paths that directly connect samples from the prior distribution to corresponding samples in the target distribution. During training, a training example is constructed by first sampling a target sample ùê±1subscriptùê±1\\mathbf{x}_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, drawing noise ùê±0‚àºùí©‚Å¢(0,1)similar-tosubscriptùê±0ùí©01\\mathbf{x}_{0}\\sim\\mathcal{N}(0,1)bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àº caligraphic_N ( 0 , 1 ) from the standard normal distribution, and selecting a timestep t‚àà[0,1]ùë°01t\\in[0,1]italic_t ‚àà [ 0 , 1 ]. The training example is then defined through a linear interpolation: ùê±t=t‚ãÖùê±1+(1‚àít)‚ãÖùê±0subscriptùê±ùë°‚ãÖùë°subscriptùê±1‚ãÖ1ùë°subscriptùê±0\\displaystyle\\mathbf{x}_{t}=t\\cdot\\mathbf{x}_{1}+(1-t)\\cdot\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t ‚ãÖ bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ( 1 - italic_t ) ‚ãÖ bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (1) The model is trained to approximate the velocity defined by an ordinary differential equation (ODE), ùêØt=d‚Å¢ùê±td‚Å¢tsubscriptùêØùë°ùëësubscriptùê±ùë°ùëëùë°\\mathbf{v}_{t}=\\frac{d\\mathbf{x}_{t}}{dt}bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_d bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_d italic_t end_ARG, enabling it to effectively guide the transformation from the intermediate sample ùê±tsubscriptùê±ùë°\\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to the real data sample ùê±1subscriptùê±1\\mathbf{x}_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. A notable advantage of Flow Matching is its ability to interpolate between two arbitrary distributions, not restricted to using only a standard Gaussian as the source domain. Consequently, in image generation tasks, Flow Matching extends beyond noise-to-image scenarios and can be effectively employed for diverse applications such as image-to-image translation. Figure 2: PixelFlow for cascaded image generation from pixel space. We partition the entire generation procedure into series resolution stages. At the beginning of each resolution stage, we upscale the relatively noisy results from the preceding stage and use them as the starting point for the current stage. Consequently, as the resolution enhances, more refined samples can be obtained."
  },
  {
    "title": "3.2 Multi-Scale Generation in Pixel Space",
    "level": 3,
    "content": "PixelFlow generates images by progressively increasing their resolution through a multistage denoising process. To enable this, we construct a multi-scale representation of the target image ùê±1subscriptùê±1\\mathbf{x}_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT by recursively downsampling it by a factor of 2 at each scale. As illustrated in Figure 2, PixelFlow divides the image generation process into SùëÜSitalic_S stages. Each stage s‚àà0,1,‚Ä¶,S‚àí1ùë†01‚Ä¶ùëÜ1s\\in{0,1,...,S-1}italic_s ‚àà 0 , 1 , ‚Ä¶ , italic_S - 1 operates over a time interval defined by the start and end states (ùê±‚Å¢t0s,ùê±‚Å¢t1s)ùê±superscriptsubscriptùë°0ùë†ùê±superscriptsubscriptùë°1ùë†(\\mathbf{x}{t_{0}^{s}},\\mathbf{x}{t_{1}^{s}})( bold_x italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , bold_x italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ). In the degenerate case where S=1ùëÜ1S=1italic_S = 1, PixelFlow reduces to a standard single-stage flow matching approach for image generation, similar to recent works [42, 17], but crucially operates in pixel space rather than latent space. For each stage sùë†sitalic_s, we define the starting and ending states as follows: Start:ùê±t0sStart:subscriptùê±superscriptsubscriptùë°0ùë†\\displaystyle\\text{\\small Start:}\\quad\\mathbf{x}_{t_{0}^{s}}Start: bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT =t0s‚ãÖUp‚Å¢(Down‚Å¢(ùê±1,2s+1))+(1‚àít0s)‚ãÖœµabsent‚ãÖsuperscriptsubscriptùë°0ùë†UpDownsubscriptùê±1superscript2ùë†1‚ãÖ1superscriptsubscriptùë°0ùë†italic-œµ\\displaystyle=t_{0}^{s}\\cdot\\textsf{\\footnotesize Up}(\\textsf{\\footnotesize Down% }(\\mathbf{x}_{1},2^{s+1}))+(1-t_{0}^{s})\\cdot\\epsilon= italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ‚ãÖ Up ( Down ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 2 start_POSTSUPERSCRIPT italic_s + 1 end_POSTSUPERSCRIPT ) ) + ( 1 - italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) ‚ãÖ italic_œµ (2) End:ùê±t1sEnd:subscriptùê±superscriptsubscriptùë°1ùë†\\displaystyle\\text{\\small End:}\\quad\\mathbf{x}_{t_{1}^{s}}End: bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT =t1s‚ãÖDown‚Å¢(ùê±1,2s)+(1‚àít1s)‚ãÖœµ,absent‚ãÖsuperscriptsubscriptùë°1ùë†Downsubscriptùê±1superscript2ùë†‚ãÖ1superscriptsubscriptùë°1ùë†italic-œµ\\displaystyle=t_{1}^{s}\\cdot\\textsf{\\footnotesize Down}(\\mathbf{x}_{1},2^{s})+% (1-t_{1}^{s})\\cdot\\epsilon,= italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ‚ãÖ Down ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) + ( 1 - italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) ‚ãÖ italic_œµ , (3) where Down(‚ãÖ‚ãÖ\\cdot‚ãÖ) and Up(‚ãÖ‚ãÖ\\cdot‚ãÖ) denote the downsampling and upsampling operations, respectively. Unless otherwise stated, we adopt bilinear interpolation for downsampling and nearest neighbor for upsampling. To train the model, we sample intermediate representations by linearly interpolating between the start and end states: ùê±tœÑs=œÑ‚ãÖùê±t1s+(1‚àíœÑ)‚ãÖùê±t0s,subscriptùê±superscriptsubscriptùë°ùúèùë†‚ãÖùúèsubscriptùê±superscriptsubscriptùë°1ùë†‚ãÖ1ùúèsubscriptùê±superscriptsubscriptùë°0ùë†\\displaystyle\\mathbf{x}_{t_{\\tau}^{s}}=\\tau\\cdot\\mathbf{x}_{t_{1}^{s}}+(1-\\tau% )\\cdot\\mathbf{x}_{t_{0}^{s}},bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_œÑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_œÑ ‚ãÖ bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + ( 1 - italic_œÑ ) ‚ãÖ bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , (4) where œÑ=t‚àít0st1s‚àít0sùúèùë°superscriptsubscriptùë°0ùë†superscriptsubscriptùë°1ùë†superscriptsubscriptùë°0ùë†\\tau=\\frac{t-t_{0}^{s}}{t_{1}^{s}-t_{0}^{s}}italic_œÑ = divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT - italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG is the rescaled timestep [65, 29] within the sùë†sitalic_s-th stage. Then our objective is to train a model ŒºŒ∏‚Å¢(‚ãÖ)subscriptùúáùúÉ‚ãÖ\\mu_{\\theta}(\\cdot)italic_Œº start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ ) to predict the velocity ŒºŒ∏‚Å¢(ùê±tœÑs,œÑ)subscriptùúáùúÉsubscriptùê±superscriptsubscriptùë°ùúèùë†ùúè\\mu_{\\theta}(\\mathbf{x}_{t_{\\tau}^{s},\\tau})italic_Œº start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_œÑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_œÑ end_POSTSUBSCRIPT ) with target as ùêØt=ùê±t1s‚àíùê±t0ssubscriptùêØùë°subscriptùê±superscriptsubscriptùë°1ùë†subscriptùê±superscriptsubscriptùë°0ùë†\\mathbf{v}_{t}=\\mathbf{x}_{t_{1}^{s}}-\\mathbf{x}_{t_{0}^{s}}bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT - bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. We use the mean squared error (MSE) loss, formally represented as: ùîºs,t,(ùê±t1s,ùê±t1s)‚Å¢‚ÄñŒºŒ∏‚Å¢(ùê±tœÑs,œÑ)‚àíùêØt‚Äñ2subscriptùîºùë†ùë°subscriptùê±superscriptsubscriptùë°1ùë†subscriptùê±superscriptsubscriptùë°1ùë†superscriptnormsubscriptùúáùúÉsubscriptùê±superscriptsubscriptùë°ùúèùë†ùúèsubscriptùêØùë°2\\displaystyle\\mathbb{E}_{s,t,(\\mathbf{x}_{t_{1}^{s}},\\mathbf{x}_{t_{1}^{s}})}|% |\\mu_{\\theta}(\\mathbf{x}_{t_{\\tau}^{s},\\tau})-\\mathbf{v}_{t}||^{2}blackboard_E start_POSTSUBSCRIPT italic_s , italic_t , ( bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT | | italic_Œº start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_œÑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_œÑ end_POSTSUBSCRIPT ) - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (5)"
  },
  {
    "title": "3.3 Model Architecture",
    "level": 3,
    "content": "We instantiate ŒºŒ∏‚Å¢(‚ãÖ)subscriptùúáùúÉ‚ãÖ\\mu_{\\theta}(\\cdot)italic_Œº start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ ) using a Transformer-based architecture [62], chosen for its simplicity, scalability, and effectiveness in generative modeling. Specifically, our implementation is based on the standard Diffusion Transformer (DiT) [45], employing XL-scale configurations across all experiments. To better align with the PixelFlow framework, we introduce several modifications, as detailed below."
  },
  {
    "title": "Patchify.",
    "level": 4,
    "content": "Following the Vision Transformer (ViT) design [15, 45], the first layer of PixelFlow is a patch embedding layer, which converts the spatial representation of the input image into a 1D sequence of tokens via a linear projection. In contrast to prior latent transformers [45, 42, 17] that operate on VAE-encoded latents, PixelFlow directly tokenizes raw pixel inputs. To support efficient attention across multiple resolutions within a batch, we apply a sequence packing strategy [11], concatenating flattened token sequences of varying lengths‚Äîcorresponding to different resolutions‚Äîalong the sequence dimension."
  },
  {
    "title": "RoPE.",
    "level": 4,
    "content": "After patchfying, we replace the original sincos positional encoding [45] with RoPE [58] to better handle varying image resolutions. RoPE has shown strong performance in enabling length extrapolation, particularly in large language models. To adapt it for 2D image data, we apply 2D-RoPE by independently applying 1D-RoPE to the height and width dimensions, with each dimension occupying half of the hidden state. Figure 3: Visualization of intermediate result of cascaded stages. We extract the intermediate results from each of the four stages for direct visualization. We observed a clear denoising process at various resolution stages."
  },
  {
    "title": "Resolution Embedding.",
    "level": 4,
    "content": "Since PixelFlow operates across multiple resolutions using a shared set of model parameters, we introduce an additional resolution embedding to distinguish between resolutions. Specifically, we use the absolute resolution of the feature map after patch embedding as a conditional signal. This signal is encoded using sinusoidal position embedding [62] and added to the timestep embedding before being passed into the model."
  },
  {
    "title": "Text-to-Image Generation.",
    "level": 4,
    "content": "While class-conditional image generation typically integrates conditioning information through adaptive layer normalization (adaLN)[45], we extend PixelFlow to support text-to-image generation by introducing a cross-attention layer after each self-attention layer within every Transformer block [7, 6]. This design allows the model to effectively align visual features with the textual input at every stage of the generation process. Following recent work [59, 8], we adopt the Flan-T5-XL language model [10] to extract rich text embeddings, which serve as conditioning signals throughout the network."
  },
  {
    "title": "3.4 Training and Inference",
    "level": 3,
    "content": "To facilitate efficient training, we uniformly sample training examples from all resolution stages using the interpolation scheme defined in Equation 4. Additionally, we employ the sequence packing technique [11], which enables joint training of scale-variant examples within a single mini-batch, improving both efficiency and scalability. During inference, the generation process begins with pure Gaussian noise at the lowest resolution and progressively transitions to higher resolutions through multiple stages. Within each resolution stage, we apply standard flow-based sampling, using either the Euler discrete sampler [17] or the Dopri5 solver, depending on the desired trade-off between speed and accuracy. To ensure smooth and coherent transitions across scales, we adopt an renoising strategy [60, 29], which effectively mitigates the jumping point issue [4] often observed in multi-scale generation pipelines."
  },
  {
    "title": "4 Experiments",
    "level": 2,
    "content": "In this section, we first detail our experimental setup in Sec. 4.1. Subsequently, we analyze key components of our approach, including model design (Sec. 4.2) and inference configurations (Sec. 4.3). Finally, we benchmark PixelFlow against state-of-the-art methods on class- (Sec. 4.4) and text-to-image (Sec. 4.5) generation tasks."
  },
  {
    "title": "4.1 Experimental Setup",
    "level": 3,
    "content": "We evaluate PixelFlow for class-conditional image generation on the ImageNet-1K [12] dataset. Unless stated otherwise, we train PixelFlow at 256√ó\\times√ó256 resolution. All models are trained using the AdamW optimizer [32, 41] with a constant learning rate of 1√ó10‚àí41superscript1041\\times 10^{-4}1 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT. Performance is primarily measured by Fr√©chet Inception Distance (FID) using the standard evaluation toolkit111https://github.com/openai/guided-diffusion. We also report Inception Score (IS) [53], sFID [43], and Precision/Recall [33]. For text-conditional image generation, we progressively train PixelFlow from 256√ó\\times√ó256 up to 1024√ó\\times√ó1024 resolution. We include qualitative comparisons with current start-of-the-art generative models, along with quantitative assessments on popular benchmarks such as T2I-CompBench [27], GenEval [19], and DPG-Bench [26]."
  },
  {
    "title": "Kickoff sequence length.",
    "level": 4,
    "content": "In principle, PixelFlow can be trained to progressively increase resolution from very low resolution (e.g., 1√ó1111\\times 11 √ó 1) up to the target resolution. However, this approach is inefficient in practice, as tokens at extremely low resolutions convey limited meaningful information. Furthermore, allocating excessive timesteps to very short sequences underutilizes the computational capacity of modern GPUs, resulting in decreased model FLOPS utilizationt. Therefore, we explore how varying the resolution at which image generation begins, which we call kickoff image resolution, impacts overall performance. kickoff seq. len. FID ‚Üì‚Üì\\downarrow‚Üì sFID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë Precision ‚Üë‚Üë\\uparrow‚Üë Recall ‚Üë‚Üë\\uparrow‚Üë 32√ó\\times√ó32 3.34 6.11 84.75 0.78 0.57 8√ó\\times√ó8 3.21 6.23 78.50 0.78 0.56 2√ó\\times√ó2 3.49 6.45 67.81 0.78 0.54 Table 1: Effect of kickoff sequence length. All models are trained with 600k iterations on ImageNet-1K. Patch size is 2√ó\\times√ó2 and target image resolution is 64√ó\\times√ó64. For our transformer-based backbone, the number of tokens involved in attention operations is determined by the raw image resolution and the patch size. In this experiment, we maintain a consistent patch size of 2√ó\\times√ó2 [45], making the kickoff sequence length directly dependent on the kickoff image resolution. Specifically, we evaluate three kickoff sequence length‚Äî2√ó\\times√ó2, 8√ó\\times√ó8, and 32√ó\\times√ó32‚Äîwhile keeping the target resolution fixed at 64√ó\\times√ó64. Notably, the 32√ó\\times√ó32 setting represents a vanilla pixel-based approach without cascading across resolutions. As shown in Table 1, among these configurations, the 8√ó\\times√ó8 kickoff sequence length achieves comparable or even slightly improved FID compared to the 32√ó\\times√ó32 baseline. This suggests that initiating generation from an appropriately smaller resolution and progressively scaling up can maintain generation quality while improving computational efficiency by allocating fewer computations to the largest resolution stage. Conversely, reducing the kickoff sequence length further to 2√ó\\times√ó2 results in a performance degradation, likely because tokens at extremely low resolutions provide limited useful information and insufficient guidance for subsequent generation steps. Taking into account both generation quality and computational efficiency, we therefore adopt 8√ó\\times√ó8 as our default kickoff sequence length. patch size FID ‚Üì‚Üì\\downarrow‚Üì sFID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë Precision ‚Üë‚Üë\\uparrow‚Üë Recall ‚Üë‚Üë\\uparrow‚Üë speed‚Ä† target res. 64√ó\\times√ó64; kickoff seq. len. 2√ó\\times√ó2; 600K iters 2√ó\\times√ó2 3.49 6.45 67.81 0.78 0.54 1.28 4√ó\\times√ó4 3.41 5.52 68.83 0.77 0.56 0.58 target res. 256√ó\\times√ó256; kickoff seq. len. 2√ó\\times√ó2; 100K iters 2√ó\\times√ó2 28.50 6.40 47.37 0.58 0.53 30.88 4√ó\\times√ó4 33.17 7.71 42.29 0.57 0.52 7.31 8√ó\\times√ó8 47.50 9.63 31.19 0.45 0.50 3.96 target res. 256√ó\\times√ó256; kickoff seq. len. 2√ó\\times√ó2; 1600K iters; EMA 4√ó\\times√ó4 2.81 5.48 251.79 0.82 0.55 7.31 8√ó\\times√ó8 4.65 5.42 195.50 0.79 0.54 3.96 Table 2: Effect of patch size. All models have a kickoff sequence length of 2√ó\\times√ó2. Upper: target resolution of 64√ó\\times√ó64; Middle: target resolution of 256√ó\\times√ó256 resolution, training with 100K iterations due to computational constraints of patch size 2√ó\\times√ó2; Bottom: Extended training to 1600K iterations at 256√ó\\times√ó256 resolution. ‚Ä†Speed measured as number of seconds per sample on a single GPU with a batchsize of 50."
  },
  {
    "title": "Patch size.",
    "level": 4,
    "content": "Next, we investigate the impact of patch size on model performance while maintaining a kickoff sequence length of 2√ó\\times√ó2. Initially, we experiment with a target resolution of 64√ó\\times√ó64 and compare two patch sizes‚Äî2√ó\\times√ó2 and 4√ó\\times√ó4‚Äîwith results presented in the upper section of Table 2. We observe that PixelFlow achieves very similar performance across these two settings, with the 4√ó\\times√ó4 patch slightly outperforming the 2√ó\\times√ó2 patch on four out of five evaluation metrics. Furthermore, using a patch size of 4√ó\\times√ó4 eliminates the highest-resolution stage required by the 2√ó\\times√ó2 patch size configuration, thus improving efficiency. When scaling to a larger target resolution (i.e., 256√ó\\times√ó256), employing a patch size of 2√ó\\times√ó2 becomes computationally infeasible due to substantial resource demands, limiting our experiments to only 100K training iterations (middle section of Table 2). This constraint necessitates adopting larger patch sizes. Although increasing the patch size further to 8√ó\\times√ó8 significantly enhances computational efficiency, it leads to a noticeable drop in performance quality. Moreover, this performance gap persists even after extended training (1600K iterations), as shown in the bottom section of Table 2. Considering both generation quality and computational cost, we therefore select a patch size of 4√ó\\times√ó4 as our default setting. step FID ‚Üì‚Üì\\downarrow‚Üì sFID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë Precision ‚Üë‚Üë\\uparrow‚Üë Recall ‚Üë‚Üë\\uparrow‚Üë 10 3.39 5.98 255.27 0.80 0.54 20 2.53 5.53 272.13 0.82 0.56 30 2.51 5.82 274.92 0.82 0.56 40 2.55 6.58 272.68 0.81 0.56 (a) Effect of number of steps per stage. CFG is a global constant value 1.50, sample function is Euler. solver FID ‚Üì‚Üì\\downarrow‚Üì sFID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë Precision ‚Üë‚Üë\\uparrow‚Üë Recall ‚Üë‚Üë\\uparrow‚Üë Euler 2.51 5.82 274.92 0.82 0.56 Dopri5 2.43 5.38 282.20 0.83 0.56 (b) Effect of sample function. CFG is a global constant value 1.50, the number of steps per stage is 30 in Euler, the absolute tolerance is 1e-6 in Dopri5. cfg schedule cfg max value FID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë global constant 1.50 2.43 282.2 stage-wise constant 2.40 1.98 282.1 (c) Effect of classifier-free guidance (CFG) setting. Sample function is Dopri5 with absolute tolerance 1e-6. Table 3: Inference Setting. The best performance is obtained by CFG step-wise constant with maximum value 2.40 and Dopri5 sample function. Figure 4: Qualitative results of class-conditional image generation of PixelFlow. All images are 256√ó\\times√ó256 resolution."
  },
  {
    "title": "4.3 Inference Schedule",
    "level": 3,
    "content": "In Table 3, we provide a detailed analysis of the inference configuration space, including the number of inference steps at each resolution stage, the choice of ODE solver, and the scheduling of classifier-free guidance (CFG)."
  },
  {
    "title": "Number of sample steps.",
    "level": 4,
    "content": "In Table 3(a), we evaluate the impact of the number of inference steps per resolution stage on generation quality. As the number of steps increases, we observe consistent improvements in FID, sFID, and IS, with the best overall performance achieved at 30 steps. Beyond this point, gains saturate and even slightly decline, indicating diminishing returns. A notable advantage of PixelFlow is its flexibility in assigning different numbers of sampling steps to each resolution stage during inference. This adaptive configuration allows fine-grained control over the sampling process, enabling performance‚Äìefficiency trade-offs. Moving beyond a uniform setting and exploring more granular stage-specific step allocations holds the potential for further performance enhancements."
  },
  {
    "title": "ODE Solver.",
    "level": 4,
    "content": "We further investigate the effect of the ODE solver type on generation quality. As shown in Table 3(b), we compare the first-order Euler solver with the adaptive higher-order Dormand‚ÄìPrince (Dopri5) solver [14]. The results indicate that Dopri5 consistently outperforms Euler across most evaluation metrics, achieving lower FID and sFID scores, a higher Inception Score, and slightly better precision, while maintaining similar recall. This demonstrates that more accurate and adaptive solvers, such as Dopri5, can better capture the generative dynamics, leading to higher-quality samples‚Äîthough often with increased computational cost."
  },
  {
    "title": "CFG Schedule.",
    "level": 4,
    "content": "Inspired by the recent process [5, 34, 63], we propose a stage-wise CFG schedule, where different stages apply different CFG values, and from the early stage to the later stage, the value increases from 1 to CFGmaxsubscriptCFGmax\\text{CFG}_{\\text{max}}CFG start_POSTSUBSCRIPT max end_POSTSUBSCRIPT. In the condition of 4 stages, we find that 0, 1/6, 2/3 and 1 of the (CFGmax‚àí1)subscriptCFGmax1(\\text{CFG}_{\\text{max}}-1)( CFG start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - 1 ) give the best FID performance. The comparison between global constant CFG and stage-wise CFG is shown in Table 3(c), in which we search the best CFG value for each method. Our proposed stage-wise CFG boosts the FID performance from 2.43 to 1.98. Model FID ‚Üì‚Üì\\downarrow‚Üì sFID ‚Üì‚Üì\\downarrow‚Üì IS ‚Üë‚Üë\\uparrow‚Üë Precision ‚Üë‚Üë\\uparrow‚Üë Recall ‚Üë‚Üë\\uparrow‚Üë Latent Space LDM-4-G [50] 3.60 - 247.7 0.87 0.48 DiT-XL/2 [45] 2.27 4.60 278.2 0.83 0.57 SiT-XL/2 [42] 2.06 4.49 277.5 0.83 0.59 Pixel Space ADM-G [13] 4.59 5.25 186.7 0.82 0.52 ADM-U [13] 3.94 6.14 215.8 0.83 0.53 CDM [22] 4.88 - 158.7 - - RIN [28, 9] 3.42 - 182.0 - - SD, U-ViT-L [24] 2.77 - 211.8 - - MDM [20] 3.51 - - - - StyleGAN-XL [54] 2.30 4.02 265.1 0.78 0.53 VDM++ [31] 2.12 - 267.7 - - PaGoDA [30] 1.56 - 259.6 - 0.59 SiD2 [25] 1.38 - - - - JetFormer [61] 6.64 - - 0.69 0.56 FractalMAR-H [37] 6.15 - 348.9 0.81 0.46 PixelFlow (ours) 1.98 5.83 282.1 0.81 0.60 Table 4: Comparisons on class-conditional image generation on ImageNet 256√ó\\times√ó256. PixelFlow achieves competitive performance compared with latent space based models."
  },
  {
    "title": "4.4 Comparison on ImageNet Benchmark",
    "level": 3,
    "content": "In Table 4, we compare PixelFlow with both latent-based and pixel-based image generation models on the ImageNet 256√ó\\times√ó256 benchmark. PixelFlow achieves an FID of 1.98, representing highly competitive performance relative to state-of-the-art latent-space methods. For instance, it outperforms LDM [50] (FID 3.60), DiT [45] (FID 2.27), and SiT [42] (FID 2.06), while achieving comparable IS and recall scores. These results highlight the effectiveness of our design, suggesting that PixelFlow can serve as a strong prototype for high-quality visual generation systems. Compared with recent pixel-based models, PixelFlow achieves superior sample quality. It notably outperforms FractalMAR-H [37], and also delivers competitive or better results than strong baselines like ADM-U [13], SiD2 [25], and VDM++ [31]. We visualize class-conditional image generation of PixelFlow at 256√ó\\times√ó256 resolution in Figure 4. We can observe our model is able to generate images of high visual quality across a wide range of classes."
  },
  {
    "title": "Settings.",
    "level": 4,
    "content": "We adopt a two-stage training strategy for text-to-image generation of PixelFlow. First, the model is initialized with an ImageNet-pretrained checkpoint at a resolution of 256√ó\\times√ó256 and trained on a subset of the LAION dataset [55] at the same resolution. In the second stage, we fine-tune the model on a curated set of high-aesthetic-quality images at a higher resolution of 512√ó\\times√ó512. All reported results for PixelFlow are based on this final 512√ó\\times√ó512 resolution model. Method GenEval T2I-CompBench DPG Overall Color Shape Texture Bench SDv1.5 [50] 0.43 0.3730 0.3646 0.4219 63.18 DALL-E 2 [49] 0.52 0.5750 0.5464 0.6374 - SDv2.1 [50] 0.50 0.5694 0.4495 0.4982 - SDXL [47] 0.55 0.6369 0.5408 0.5637 74.65 PixArt-Œ±ùõº\\alphaitalic_Œ± [6] 0.48 0.6886 0.5582 0.7044 71.11 DALL-E 3 [3] 0.67‚Ä† 0.8110‚Ä† 0.6750‚Ä† 0.8070‚Ä† 83.50‚Ä† GenTron [7] - 0.7674 0.5700 0.7150 - SD3 [17] 0.74 - - - - Transfusion [70] 0.63 - - - - LlamaGen [59] 0.32 - - - - Emu 3 [64] 0.66‚Ä† 0.7913‚Ä† 0.5846‚Ä† 0.7422‚Ä† 80.60 PixelFlow (ours) 0.60 0.7578 0.4529 0.6006 77.93 0.64‚Ä† 0.7689‚Ä† 0.5059‚Ä† 0.6273‚Ä† Table 5: Comparison with state-of-the-art models on text-to-image generation benchmarks. We evaluate on GenEval [19], T2I-CompBench [27] and DPG-Bench [26]. We use ‚Ä†‚Ä†\\dagger‚Ä† to indicate the result with prompt rewriting. Figure 5: Qualitative results of text-conditional generation of PixelFlow. All images are 512√ó\\times√ó512 resolution. Key components of the prompt are highlighted in RED. To comprehensively evaluate the performance of PixelFlow-T2I in text-to-image generation, we employ three widely recognized benchmarks, each targeting a different facet of compositional understanding: T2I-CompBench [27] assesses alignment between generated images and complex semantic relationships in text. We evaluate three tasks‚Äîcolor, shape, and texture binding‚Äîby generating five images per prompt across 300 prompts per sub-task. Alignment is measured using BLIP-VQA[36]; GenEval [19] evaluates compositional aspects such as coherence and spatial arrangement. We generate over 2,000 images from 553 prompts and report the average performance across tasks; DPG-Bench [26] focuses on complex textual descriptions, with 4,000 images generated from 1,065 prompts and results averaged across tasks."
  },
  {
    "title": "Quantitative results.",
    "level": 4,
    "content": "As shown in Table 5, PixelFlow achieves competitive performance across all benchmarks, demonstrating strong compositional understanding in free-form text-to-image generation. It performs particularly well on T2I-CompBench, with high scores in color and texture binding, and solid results on GenEval (0.64) and DPG-Bench (77.93), surpassing many established models. These results underscore PixelFlow as a promising direction for pixel-space image generation conditioned on natural language‚Äîshowcasing its potential for open-ended, text-driven image synthesis."
  },
  {
    "title": "Visualization.",
    "level": 4,
    "content": "We visualize the intermediate results during the sampling process in Figure 3, specifically showing the final step of each resolution stage. As resolution increases, a clear denoising trend emerges‚Äîimages become progressively cleaner and less noisy at each stage. Additional generated samples along with their input text prompts are shown in Figure 5 (512√ó\\times√ó512) and Figure 6 (1024√ó\\times√ó1024). PixelFlow demonstrates high visual fidelity and strong text-image alignment, effectively capturing key visual elements and their relationships from complex prompts. Notably, it generates fine-grained details‚Äîsuch as animal fur, human hair, and hat textures‚Äîhighlighting its strong attention to detail in pixel space. Figure 6: Qualitative samples of PixelFlow. We present the generated images of 1024√ó\\times√ó1024 resolution. Key words are highlighted in RED."
  },
  {
    "title": "5 Conclusion",
    "level": 2,
    "content": "We introduce PixelFlow, a novel image generation model that re-think the predominance of latent space based models by directly operating on raw pixel space. By directly transforming between different resolution stages, our model exhibits a compelling advantage in simplicity and end-to-end trainability. On both class-conditional image generation and text-to-image generation benchmarks, PixelFlow has been proven to demonstrate competitive image generation capabilities compared to popular latent space-based methods. We hope that this new perspective will inspire future research in visual generation models."
  },
  {
    "title": "Limitations",
    "level": 4,
    "content": "Despite its advantages, PixelFlow still faces certain limitations. Although the model avoids full-resolution computation across all stages, the final stage requires full-resolution attention, which accounts for roughly 80% of the total inference time. Moreover, we observe that training convergence slows as the sequence length increases. Addressing these challenges presents opportunities for future improvements in efficiency and scalability."
  },
  {
    "title": "References",
    "level": 2,
    "content": "Albergo and Vanden-Eijnden [2023] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. Balaji et al. [2022] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Betker et al. [2023] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Campbell et al. [2023] Andrew Campbell, William Harvey, Christian Dietrich Weilbach, Valentin De Bortoli, Tom Rainforth, and Arnaud Doucet. Trans-dimensional generative modeling via jump diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Chang et al. [2022] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11315‚Äì11325, 2022. Chen et al. [2023] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-a‚Å¢l‚Å¢p‚Å¢h‚Å¢aùëéùëôùëù‚Ñéùëéalphaitalic_a italic_l italic_p italic_h italic_a: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen et al. [2024] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6441‚Äì6451, 2024. Chen et al. [2025] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. Chen [2023] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023. Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1‚Äì53, 2024. Dehghani et al. [2024] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n‚Äôpack: Navit, a vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255. Ieee, 2009. Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780‚Äì8794, 2021. Dormand and Prince [1980] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):19‚Äì26, 1980. Dosovitskiy [2020] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Esser et al. [2021] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873‚Äì12883, 2021. Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Evans et al. [2024] Zach Evans, CJ Carr, Josiah Taylor, Scott H Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. Ghosh et al. [2024] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Gu et al. [2023] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua M Susskind, and Navdeep Jaitly. Matryoshka diffusion models. In The Twelfth International Conference on Learning Representations, 2023. Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho et al. [2022] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1‚Äì33, 2022. Hong et al. [2022] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Hoogeboom et al. [2023] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 13213‚Äì13232. PMLR, 2023. Hoogeboom et al. [2024] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. Hu et al. [2024] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Huang et al. [2023] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:78723‚Äì78747, 2023. Jabri et al. [2022] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. Jin et al. [2024] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. Kim et al. [2024] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of a one-step generator from a low-resolution diffusion teacher. arXiv preprint arXiv:2405.14822, 2024. Kingma and Gao [2024] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Kynk√§√§nniemi et al. [2019] Tuomas Kynk√§√§nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Kynk√§√§nniemi et al. [2024] Tuomas Kynk√§√§nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in a limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Labs [2024] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888‚Äì12900. PMLR, 2022. Li et al. [2025] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. Lipman et al. [2023] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Liu et al. [2023a] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023a. Liu et al. [2023b] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023b. Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Ma et al. [2024] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Nash et al. [2021] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. NVIDIA [2024] NVIDIA. Edify image: High-quality image generation with pixel space laplacian diffusion model. arXiv preprint arXiv:2411.07126, 2024. Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195‚Äì4205, 2023. Pernias et al. [2023] Pablo Pernias, Dominic Rampas, Mats L Richter, Christopher J Pal, and Marc Aubreville. W√ºrstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv preprint arXiv:2306.00637, 2023. Podell et al. [2023] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821‚Äì8831. Pmlr, 2021. Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022. Saharia et al. [2022a] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479‚Äì36494, 2022a. Saharia et al. [2022b] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):4713‚Äì4726, 2022b. Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sauer et al. [2022] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1‚Äì10, 2022. Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278‚Äì25294, 2022. Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256‚Äì2265. PMLR, 2015. Stan et al. [2023] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, et al. Ldm3d: Latent diffusion model for 3d. arXiv preprint arXiv:2305.10853, 2023. Su et al. [2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sun et al. [2024] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Teng et al. [2023] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Tschannen et al. [2024] Michael Tschannen, Andr√© Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024. Vaswani et al. [2017] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. Wang et al. [2024a] Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fern√°ndez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. arXiv preprint arXiv:2404.13040, 2024a. Wang et al. [2024b] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Yan et al. [2024] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, qiang liu, and Jiashi Feng. PeRFlow: Piecewise rectified flow as universal plug-and-play accelerator. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yang et al. [2024] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Zeng et al. [2022] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing Systems, 2022. Zhai et al. [2024] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. Zhang et al. [2025] Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, and Ping Luo. Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation. arXiv preprint arXiv:2502.05179, 2025. Zhou et al. [2024] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
  },
  {
    "title": "Instructions for reporting errors",
    "level": 2,
    "content": "We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below: Click the \"Report Issue\" button. Open a report feedback form via keyboard, use \"Ctrl + ?\". Make a text selection and click the \"Report Issue for Selection\" button near your cursor. You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section. Our team has already identified the following issues. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all. Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions."
  }
]