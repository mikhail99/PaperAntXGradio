[
  {
    "title": "Table of Contents",
    "level": 2,
    "content": "Abstract 1 Introduction 2 Related Work Diffusion Transformers. Fast Diffusion Training. 3 Preliminary Analysis 4 Method 4.1 Condition Encoder 4.2 Velocity Decoder 4.3 Sampling acceleration Uniform Encoder Sharing. Statistic Dynamic Programming. 5 Experiment 5.1 Improved baselines 5.2 Metric comparison with baselines 5.3 System level comparision ImageNet 256×256256256256\\times 256256 × 256. ImageNet 512×512512512512\\times 512512 × 512 5.4 Acceleration by Encoder sharing 5.5 Ablations Encoder-Decoder Ratio Decoder Block types. 6 Conclusion A Model Specs B Hyper-parameters C Linear flow and Diffusion D Proof of Spectrum Autoregressive E Linear multisteps method F Classifier free guidance. References"
  },
  {
    "title": "DDT: Decoupled Diffusion Transformer",
    "level": 1,
    "content": "Shuai Wang1 Zhi Tian2 Weilin Huang2 Limin Wang 1, \\faEnvelope 1Nanjing University 2ByteDance Seed Vision https://github.com/MCG-NJU/DDT Figure 1: Our deoupled diffusion transformer (DDT-XL/2) achieves a SoTA 1.31 FID under 256 epochs. Our decoupled diffusion transformer models incorporate a condition encoder to extract semantic self-conditions and a velocity decoder to decode velocity. †† \\faEnvelope : Corresponding author (lmwang@nju.edu.cn)."
  },
  {
    "title": "Abstract",
    "level": 6,
    "content": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new Decoupled Diffusion Transformer (DDT), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256×256256256256\\times 256256 × 256, Our DDT-XL/2 achieves a new state-of-the-art performance of 1.31 FID (nearly 4×4\\times4 × faster training convergence compared to previous diffusion transformers). For ImageNet 512×512512512512\\times 512512 × 512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies."
  },
  {
    "title": "1 Introduction",
    "level": 2,
    "content": "Image generation is a fundamental task in computer vision research, which aims at capturing the inherent data distribution of original image datasets and generating high-quality synthetic images through distribution sampling. Diffusion models [19, 41, 21, 30, 29] have recently emerged as highly promising solutions to learn the underlying data distribution in image generation, outperforming the GAN-based models [3, 40] and Auto-Regressive models [5, 43, 51]. The diffusion forward process gradually adds Gaussian noise to the pristine data following an SDE forward schedule [19, 41, 21]. The denoising process learns the score estimation from this corruption process. Once the score function is accurately learned, data samples can be synthesized by numerically solving the reverse SDE [41, 21, 30, 29]. Diffusion Transformers [36, 32] introduce the transformer architecture into diffusion models to replace the traditionally dominant UNet-based model [2, 10]. Empirical evidence suggests that, given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections [36]. Nevertheless, their slow convergence rate still poses great challenge for developing new models due to the high cost. In this paper, we want to tackle the aforementioned major disadvantages from a model design perspective. Classic computer vision algorithms [4, 23, 17] strategically employ encoder-decoder architectures, prioritizing large encoders for rich feature extraction and lightweight decoders for efficient inference, while contemporary diffusion models predominantly rely on conventional decoder-only structures. We systematically investigate the underexplored potential of decoupled encoder-decoder designs in diffusion transformers, by answering the question of can decoupled encoder-decoder transformer unlock the capability of accelerated convergence and enhanced sample quality? Through investigation experiments, we conclude that the plain diffusion transformer has an optimization dilemma between abstract structure information extraction and detailed appearance information recovery. Further, the diffusion transformer is limited in extracting semantic representation due to the raw pixel supervision [52, 53, 28]. To address this issue, we propose a new architecture to explicitly decouple low-frequency semantic encoding and high-frequency detailed decoding through a customized encoder-decoder design. We call this encoder-decoder diffusion transformer model as DDT (Decoupled Diffusion Transformer). DDT incorporates a condition encoder to extract semantic self-condition features. The extracted self-condition is fed into a velocity decoder along with the noisy latent to regress the velocity field. To maintain the local consistency of self-condition features of adjacent steps, we employ direct supervision of representation alignment and indirect supervision from the velocity regression loss of the decoder. In the ImageNet256×256256256256\\times 256256 × 256 dataset, using the traditional off-shelf VAE [38], our decoupled diffusion transformer (DDT-XL/2) model achieves the state-of-the-art performance of 1.31 FID with interval guidance under only 256 epochs, approximately 4×4\\times4 × training acceleration compared to REPA [52]. In the ImageNet512×512512512512\\times 512512 × 512 dataset, our DDT-XL/2 model achieves 1.28 FID within 500K finetuning steps. Furthermore, our DDT achieves strong local consistency on its self-condition feature from the encoder. This property can significantly boost the inference speed by sharing the self-condition between adjacent steps. We formulate the optimal encoder sharing strategy solving as a classic minimal sum path problem by minimizing the performance drop of sharing self-condition among adjacent steps. We propose a statistic dynamic programming approach to find the optimal encoder sharing strategy with negligible second-level time cost. Compared with the naive uniform sharing, our dynamic programming delivers a minimal FID drop. Our contributions are summarized as follows. • We propose a new decoupled diffusion transformer model, which consists of a condition encoder and a velocity decoder. • We propose statistic dynamic programming to find the optimal self-condition sharing strategy to boost inference speed while keeping minimal performance down-gradation. • In the ImageNet256×256256256256\\times 256256 × 256 dataset, using tradition SDf8d4 VAE, our decoupled diffusion transformer (DDT-XL/2) model achieves the SoTA 1.31 FID with interval guidance under only 256 epochs, approximately 4×4\\times4 × training acceleration compared to REPA [52]. • In the ImageNet512×512512512512\\times 512512 × 512 dataset, our DDT-XL/2 model achieves the SoTA 1.28 FID, outperforming all previous methods with a significant margin."
  },
  {
    "title": "2 Related Work",
    "level": 2,
    "content": "Figure 2: Selected 256×256256256256\\times 256256 × 256 and 512×512512512512\\times 512512 × 512 resolution samples. Generated from DDT-XL/2 trained on ImageNet 256×256256256256\\times 256256 × 256 resolution and ImageNet 512×512512512512\\times 512512 × 512 resolution with CFG = 4.0."
  },
  {
    "title": "Diffusion Transformers.",
    "level": 4,
    "content": "The pioneering work of DiT [36] introduced transformers into diffusion models to replace the traditionally dominant UNet architecture [2, 10]. Empirical evidence demonstrates that given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections. SiT [32] further validated the transformer architecture with linear flow diffusion. Following the simplicity and scalability of the diffusion transformer [32, 36], SD3 [12], Lumina [54], and PixArt [6, 7] introduced the diffusion transformer to more advanced text-to-image areas. Moreover, recently, diffusion transformers have dominated the text-to-video area with substantiated visual and motion quality [24, 1, 20]. Our decoupled diffusion transformer (DDT) presents a new variant within the diffusion transformer family. It achieves faster convergence by decoupling the low-frequency encoding and the high-frequency decoding."
  },
  {
    "title": "Fast Diffusion Training.",
    "level": 4,
    "content": "To accelerate the training efficiency of diffusion transformers, recent advances have pursued multi-faceted optimizations. Operator-centric approaches [13, 48, 49, 45] leverage efficient attention mechanisms: linear-attention variants [13, 49, 45] reduced quadratic complexity to speed up training, while sparse-attention architectures [48] prioritized sparsely relevant token interactions. Resampling approaches [12, 16] proposed lognorm sampling [12] or loss reweighting [16] techniques to stabilize training dynamics. Representation learning enhancement approaches integrate external inductive biases: REPA [52], RCG [27] and DoD [53] borrowed vision-specific priors into diffusion training, while masked modeling techniques [14, 15] strengthened spatial reasoning by enforcing structured feature completion during denoising. Collectively, these strategies address computational, sampling, and representational bottlenecks."
  },
  {
    "title": "3 Preliminary Analysis",
    "level": 2,
    "content": "Figure 3: The reverse-SDE process (generation) of SiT-XL/2 in x𝑥xitalic_x space. There is a clear generation process from low frequency to high frequency. Most of the time is spent on generating high-frequency details (from t=0.4𝑡0.4t=0.4italic_t = 0.4 to t=1.0𝑡1.0t=1.0italic_t = 1.0). Figure 4: The FID50K metric of SiT-XL/2 for different timeshift values. We employ a 2222-nd order Adams-like solver to collect the performance. Allocating more computation at noisy steps significantly improves the performance. Linear-based flow matching [30, 29, 32] represents a specialized family of diffusion models that we focus on as our primary analytical subject due to its simplicity and efficiency. For the convenience of discussion, in certain situations, diffusion and flow-matching will be used interchangeably. In this framework, t=0𝑡0t=0italic_t = 0 corresponds to the pure noise timestep. As illustrated in Fig. 3, diffusion models perform autoregressive refinement on spectral components [37, 11]. The diffusion transformer encodes the noisy latent to capture lower-frequency semantics before decoding higher-frequency details. However, this semantics encoding process inevitably attenuates high-frequency information, creating an optimization dilemma. This observation motivates our proposal to decouple the conventional decode-only diffusion transformer into an explicit encoder-decoder architecture. Eq. 1 is directly borrowed from [37, 11], we place the proof of Eq. 1 in Appendix. According to Eq. 1, as t𝑡titalic_t increases to less noisy timesteps, semantic encoding becomes easier (due to noise reduction) while decoding complexity increases (as residual frequencies grow). Consider the worst-case scenario at denoising step t𝑡titalic_t, the diffusion transformer encodes frequencies up to fm⁢a⁢x⁢(t)subscript𝑓𝑚𝑎𝑥𝑡f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ), to progress to step s𝑠sitalic_s, it must decode a residual frequency of at least fm⁢a⁢x⁢(s)−fm⁢a⁢x⁢(t)subscript𝑓𝑚𝑎𝑥𝑠subscript𝑓𝑚𝑎𝑥𝑡f_{max}(s)-f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_s ) - italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ). Failure to decode these residual frequencies at step t𝑡titalic_t creates a critical bottleneck for progression to subsequent steps. From this perspective, if allocating more of the calculations to more noisy timesteps can lead to an improvement, it means that diffusion transformers struggle with encoding lower frequency to provide semantics. Otherwise, if allocating more of the calculations to less noisy timesteps can lead to an improvement, it means that flow-matching transformers struggle with decoding higher frequency to provide fine details. To figure out the bottom-necks of current diffusion models, we conducted a targeted experiment using SiT-XL/2 with a second-order Adams-like linear multistep solver. As shown in Fig. 4, by varying the time-shift values, we demonstrate that allocating more computation to early timesteps improves final performance compared to uniform scheduling. This reveals that diffusion models face challenges in more noisy steps. This leads to a key conclusion: Current diffusion transformers are fundamentally constrained by their low-frequency semantic encoding capacity. This insight motivates the exploration of encoder-decoder architectures with strategic encoder parameter allocation. Prior researches further support this perspective. While lightweight diffusion MLP heads demonstrate limited decoding capacity, MAR [28] overcomes this limitation through semantic latents produced by its masked backbones, enabling high-quality image generation. Similarly, REPA [52] enhances low-frequency encoding through alignment with pre-trained vision foundations [35]."
  },
  {
    "title": "Lemma 1.",
    "level": 6,
    "content": "For a linear flow-matching noise scheduler at timestep t𝑡titalic_t, let us denote Kf⁢r⁢e⁢qsubscript𝐾𝑓𝑟𝑒𝑞K_{freq}italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT as the maximum frequency of the clean data 𝐱d⁢a⁢t⁢asubscript𝐱𝑑𝑎𝑡𝑎{\\boldsymbol{x}}_{data}bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT. The maximum retained frequency in the noisy latent satisfies: fm⁢a⁢x⁢(t)>min⁡((t1−t)2,Kf⁢r⁢e⁢q).subscript𝑓𝑚𝑎𝑥𝑡superscript𝑡1𝑡2subscript𝐾𝑓𝑟𝑒𝑞f_{max}(t)>\\min\\left({\\left(\\frac{t}{1-t}\\right)}^{2},K_{freq}\\right).italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_t end_ARG start_ARG 1 - italic_t end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) . (1)"
  },
  {
    "title": "4 Method",
    "level": 2,
    "content": "Our decoupled diffusion transformer architecture comprises a condition encoder and a velocity decoder. The condition encoder extracted the low-frequency component from noisy input, class label, and timestep to serve as a self-condition for the velocity decoder; the velocity decoder processed the noisy latent with the self-condition to regress the high-frequency velocity. We train this model using the established linear flow diffusion framework. For brevity, we designate our model as DDT (Decoupled Diffusion Transformer)."
  },
  {
    "title": "4.1 Condition Encoder",
    "level": 3,
    "content": "The condition encoder mirrors the architectural design and input structure of DiT/SiT with improved micro-design. It is built with interleaved Attention and FFN blocks, without long residual connections. The encoder processes three inputs, the noisy latent 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, timestep t𝑡titalic_t, and class label y𝑦yitalic_y, to extract the self-condition feature 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT through a series of stacked Attention and FFN blocks: 𝒛t=Encoder⁢(𝒙t,t,y).subscript𝒛𝑡Encodersubscript𝒙𝑡𝑡𝑦{\\boldsymbol{z}}_{t}=\\textbf{Encoder}~{}({\\boldsymbol{x}}_{t},t,y).bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Encoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_y ) . (2) Specifically, the noisy latent 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are patchfied into continuous tokens and then fed to extract the self-condition 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with aforementioned encoder blocks. The timestep t𝑡titalic_t and class label y𝑦yitalic_y serve as external-conditioning information projected into embedding. These external-condition embeddings are progressively injected into the encoded features of 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT using AdaLN-Zero[36] within each encoder block. To maintain local consistency of 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT across adjacent timesteps, we adopt the representation alignment technique from REPA [52]. Shown in Eq. 3, this method aligns the intermediate feature 𝐡isubscript𝐡𝑖\\mathbf{h}_{i}bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT from the i𝑖iitalic_i-th layer in the self-mapping encoder with the DINOv2 representation r∗subscript𝑟r_{*}italic_r start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT. Consistent to REPA [52], the hϕsubscriptℎitalic-ϕh_{\\phi}italic_h start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT is the learnable projection MLP: ℒe⁢n⁢c=1−cos⁡(r∗,hϕ⁢(𝐡𝐢)).subscriptℒ𝑒𝑛𝑐1subscript𝑟subscriptℎitalic-ϕsubscript𝐡𝐢\\mathcal{L}_{enc}=1-\\cos(r_{*},h_{\\phi}(\\mathbf{h_{i}})).caligraphic_L start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT = 1 - roman_cos ( italic_r start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( bold_h start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT ) ) . (3) This simple regularization accelerates training convergence, as shown in REPA [52], and facilitates local consistency of 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT between adjacent steps. It allows sharing the self-condition 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT produced by the encoder between adjacent steps. Our experiments demonstrate that this encoder-sharing strategy significantly enhances inference efficiency with only negligible performance degradation. Additionally, the encoder also receives indirect supervision from the decoder, which we elaborate on later."
  },
  {
    "title": "4.2 Velocity Decoder",
    "level": 3,
    "content": "The velocity decoder adopts the same architectural design as the condition encoder and consists of several stacked interleaved Attention and FFN blocks, akin to DiT/SiT. It takes the noisy latent 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, timestep t𝑡titalic_t, and self-conditioning 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as inputs to estimate the velocity 𝒗tsubscript𝒗𝑡{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Unlike the encoder, we assume that class label information is already embedded within 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Thus, only the external-condition timestep t𝑡titalic_t and self-condition feature 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are used as condition inputs for the decoder blocks: 𝒗t=Decoder⁢(𝒙t,t,𝒛t).subscript𝒗𝑡Decodersubscript𝒙𝑡𝑡subscript𝒛𝑡{\\boldsymbol{v}}_{t}=\\textbf{Decoder}~{}({\\boldsymbol{x}}_{t},t,{\\boldsymbol{z% }}_{t}).bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Decoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (4) As demonstrated previously, to further improve consistency of self-condition 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT between adjacent steps, we employ AdaLN-Zero [36] to inject 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT into the decoder feature. The decoder is trained with the flow matching loss as shown in Eq. 5: ℒd⁢e⁢c=𝔼⁢[∫01‖(𝒙d⁢a⁢t⁢a−ϵ)−𝒗t‖2⁢dt].subscriptℒ𝑑𝑒𝑐𝔼delimited-[]superscriptsubscript01superscriptnormsubscript𝒙𝑑𝑎𝑡𝑎italic-ϵsubscript𝒗𝑡2differential-d𝑡\\mathcal{L}_{dec}=\\mathbb{E}[\\int_{0}^{1}||({\\boldsymbol{x}}_{data}-{\\epsilon}% )-{\\boldsymbol{v}}_{t}||^{2}\\mathrm{d}t].caligraphic_L start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = blackboard_E [ ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT | | ( bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT - italic_ϵ ) - bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_d italic_t ] . (5)"
  },
  {
    "title": "4.3 Sampling acceleration",
    "level": 3,
    "content": "By incorporating explicit representation alignment into the encoder and implicit self-conditioning injection into the decoder, we achieve local consistency of 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT across adjacent steps during training (shown in Fig. 5). This enables us to share 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT within a suitable local range, reducing the computational burden on the self-mapping encoder. Formally, given total inference steps N𝑁Nitalic_N and encoder computation bugets K𝐾Kitalic_K, thus the sharing ratio is 1−KN1𝐾𝑁1-\\frac{K}{N}1 - divide start_ARG italic_K end_ARG start_ARG italic_N end_ARG, we define ΦΦ\\Phiroman_Φ with |Φ|=KΦ𝐾|\\Phi|=K| roman_Φ | = italic_K as the set of timesteps where the self-condition is recalculated, as shown in Equation 6. If the current timestep t𝑡titalic_t is not in ΦΦ\\Phiroman_Φ, we reuse the previously computed 𝒛t−Δ⁢tsubscript𝒛𝑡Δ𝑡{\\boldsymbol{z}}_{t-\\Delta t}bold_italic_z start_POSTSUBSCRIPT italic_t - roman_Δ italic_t end_POSTSUBSCRIPT as 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Otherwise, we recompute 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT using the encoder and the current noisy latent 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT: 𝒛t={𝒛t−Δ⁢t,if ⁢t∉ΦEncoder⁢(𝒙t,t,y),if ⁢t∈Φsubscript𝒛𝑡casessubscript𝒛𝑡Δ𝑡if 𝑡ΦEncodersubscript𝒙𝑡𝑡𝑦if 𝑡Φ{\\boldsymbol{z}}_{t}=\\begin{cases}{\\boldsymbol{z}}_{t-\\Delta t},&\\text{if }t% \\notin\\Phi\\\\ \\textbf{Encoder}~{}({\\boldsymbol{x}}_{t},t,y),&\\text{if }t\\in\\Phi\\end{cases}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT italic_t - roman_Δ italic_t end_POSTSUBSCRIPT , end_CELL start_CELL if italic_t ∉ roman_Φ end_CELL end_ROW start_ROW start_CELL Encoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_y ) , end_CELL start_CELL if italic_t ∈ roman_Φ end_CELL end_ROW (6)"
  },
  {
    "title": "Uniform Encoder Sharing.",
    "level": 4,
    "content": "This naive approach recaluculate self-condition 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT every NK𝑁𝐾\\frac{N}{K}divide start_ARG italic_N end_ARG start_ARG italic_K end_ARG steps. Previous work, such as DeepCache [33], uses this naive handcrafted uniform ΦΦ\\Phiroman_Φ set to accelerate UNet models. However, UNet models, trained solely with a denoising loss and lacking robust representation alignment, exhibit less regularized local consistency in deeper features across adjacent steps compared to our DDT model. Also, we will propose a simple and elegant statistic dynamic programming algorithm to construct ΦΦ\\Phiroman_Φ. Our statistic dynamic programming can exploit the optimal ΦΦ\\Phiroman_Φ set optimally compared to the naive approaches [33]."
  },
  {
    "title": "Statistic Dynamic Programming.",
    "level": 4,
    "content": "We construct the statistic similarity matrix of ztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT among different steps 𝐒∈RN×N𝐒superscript𝑅𝑁𝑁\\mathbf{S}\\in R^{N\\times N}bold_S ∈ italic_R start_POSTSUPERSCRIPT italic_N × italic_N end_POSTSUPERSCRIPT using cosine distance. The optimal ΦΦ\\Phiroman_Φ set would guarantee the total similarity cost −∑kK∑i=ΦkΦk+1S⁢[Φk,i]superscriptsubscript𝑘𝐾superscriptsubscript𝑖subscriptΦ𝑘subscriptΦ𝑘1𝑆subscriptΦ𝑘𝑖-\\sum_{k}^{K}\\sum_{i=\\Phi_{k}}^{\\Phi_{k+1}}S[\\Phi_{k},i]- ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i = roman_Φ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_Φ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_S [ roman_Φ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_i ] achieves global minimal. This question is a well-formed classic minimal sum path problem, it can be solved by dynamic programming. As shown in Eq. 8, we donate 𝐂iksubscriptsuperscript𝐂𝑘𝑖\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as cost and 𝐏iksubscriptsuperscript𝐏𝑘𝑖\\mathbf{P}^{k}_{i}bold_P start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as traced path when Φk=isubscriptΦ𝑘𝑖\\Phi_{k}=iroman_Φ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_i. the state transition function from 𝐂jk−1subscriptsuperscript𝐂𝑘1𝑗\\mathbf{C}^{k-1}_{j}bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT to 𝐂iksubscriptsuperscript𝐂𝑘𝑖\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT follows: 𝐂iksubscriptsuperscript𝐂𝑘𝑖\\displaystyle\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =minj=0i⁡{𝐂jk−1−Σl=ji⁢𝐒⁢[j,l]}.absentsuperscriptsubscript𝑗0𝑖subscriptsuperscript𝐂𝑘1𝑗superscriptsubscriptΣ𝑙𝑗𝑖𝐒𝑗𝑙\\displaystyle=\\min_{j=0}^{i}\\{\\mathbf{C}^{k-1}_{j}-\\Sigma_{l=j}^{i}\\mathbf{S}[% j,l]\\}.= roman_min start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT { bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - roman_Σ start_POSTSUBSCRIPT italic_l = italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_S [ italic_j , italic_l ] } . (7) 𝐏iksubscriptsuperscript𝐏𝑘𝑖\\displaystyle\\mathbf{P}^{k}_{i}bold_P start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =argminj=0i⁡{𝐂ik−1−Σl=ji⁢𝐒⁢[j,l]}.absentsuperscriptsubscriptargmin𝑗0𝑖subscriptsuperscript𝐂𝑘1𝑖superscriptsubscriptΣ𝑙𝑗𝑖𝐒𝑗𝑙\\displaystyle=\\operatorname{argmin}_{j=0}^{i}\\{\\mathbf{C}^{k-1}_{i}-\\Sigma_{l=% j}^{i}\\mathbf{S}[j,l]\\}.= roman_argmin start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT { bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_Σ start_POSTSUBSCRIPT italic_l = italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_S [ italic_j , italic_l ] } . (8) After obtaining the cost matrix 𝐂𝐂\\mathbf{C}bold_C and tracked path 𝐏𝐏\\mathbf{P}bold_P, the optimal ΦΦ\\Phiroman_Φ can be solved by backtracking 𝐏𝐏\\mathbf{P}bold_P from 𝐏NKsuperscriptsubscript𝐏𝑁𝐾\\mathbf{P}_{N}^{K}bold_P start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT."
  },
  {
    "title": "5 Experiment",
    "level": 2,
    "content": "We conduct experiments on 256x256 ImageNet datasets. The total training batch size is set to 256. Consistent with methodological approaches such as SiT [32], DiT [36], and REPA [52], we employed the Adam optimizer with a constant learning rate of 0.0001 throughout the entire training process. To ensure a fair comparative analysis, we did not use gradient clipping and learning rate warm-up techniques. Our default training infrastructure consisted of 16×16\\times16 × or 8×8\\times8 × A100 GPUs. For sampling, we take the Euler solver with 250 steps as the default choice. As for the VAE, we take the off-shelf VAE-ft-EMA with a downsample factor of 8 from Huggingface111https://huggingface.co/stabilityai/sd-vae-ft-ema. We report FID [18], sFID [34], IS [39], Precision and Recall [25]. 256×\\times×256, w/o CFG 256×\\times×256, w/ CFG Params Epochs FID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Pre.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ FID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Pre.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ MAR-B [28] 208M 800 3.48 192.4 0.78 0.58 2.31 281.7 0.82 0.57 CausalFusion [9] 368M 800 5.12 166.1 0.73 0.66 1.94 264.4 0.82 0.59 LDM-4 [38] 400M 170 10.56 103.5 0.71 0.62 3.6 247.7 0.87 0.48 DDT-L (Ours) 458M 80 7.98 128.1 0.68 0.67 1.64 310.5 0.81 0.61 MAR-L [28] 479M 800 2.6 221.4 0.79 0.60 1.78 296.0 0.81 0.60 VAVAE [50] 675M 800 2.17 205.6 0.77 0.65 1.35 295.3 0.79 0.65 CausalFusion [9] 676M 800 3.61 180.9 0.75 0.66 1.77 282.3 0.82 0.61 ADM [10] 554M 400 10.94 - 0.69 0.63 4.59 186.7 0.82 0.52 DiT-XL [36] 675M 1400 9.62 121.5 0.67 0.67 2.27 278.2 0.83 0.57 SiT-XL [32] 675M 1400 8.3 - - - 2.06 270.3 0.82 0.59 ViT-XL [16] 451M 400 8.10 - - - 2.06 - - - U-ViT-H/2 [2] 501M 400 6.58 - - - 2.29 263.9 0.82 0.57 MaskDiT [14] 675M 1600 5.69 178.0 0.74 0.60 2.28 276.6 0.80 0.61 FlowDCN [48] 618M 400 8.36 122.5 0.69 0.65 2.00 263.1 0.82 0.58 RDM [44] 553M / 5.27 153.4 0.75 0.62 1.99 260.4 0.81 0.58 REPA [52] 675M 800 5.9 157.8 0.70 0.69 1.42 305.7 0.80 0.64 DDT-XL (Ours) 675M 80 6.62 135.2 0.69 0.67 1.52 263.7 0.78 0.63 DDT-XL (Ours) 675M 256 6.30 146.7 0.68 0.68 1.31 308.1 0.78 0.62 DDT-XL (Ours) 675M 400 6.27 154.7 0.68 0.69 1.26 310.6 0.79 0.65 Table 1: System performance comparison on ImageNet 256×256256256256\\times 256256 × 256 class-conditioned generation. Gray blocks mean the algorithm uses VAE trained or fine-tuned on ImageNet instead of the off-shelf SD-VAE-f8d4-ft-ema."
  },
  {
    "title": "5.1 Improved baselines",
    "level": 3,
    "content": "Recent architectural improvements such as SwiGLU [46, 47], RoPE [42], and RMSNorm [46, 47] have been extensively validated in the research community [8, 50, 31]. Additionally, lognorm sampling [12] has demonstrated significant benefits for training convergence. Consequently, we developed improved baseline models by incorporating these advanced techniques, drawing inspiration from recent works in the field. The performance of these improved baselines is comprehensively provided in Tab. 2. To validate the reliability of our implementation, we also reproduced the results for REPA-B/2, achieving metrics that marginally exceed those originally reported in the REPA[52]. These reproduction results provide additional confidence in the robustness of our approach. The improved baselines in our Tab. 2 consistently outperform their predecessors without REPA. However, upon implementing REPA, performance rapidly approaches a saturation point. This is particularly evident in the XL model size, where incremental technique improvements yield diminishingly small gains."
  },
  {
    "title": "5.2 Metric comparison with baselines",
    "level": 3,
    "content": "We present the performances of different-size models at 400K training steps in Tab. 2. Our diffusion encoder-decoder transformer(DDT) family demonstrates consistent and significant improvements across various model sizes. Our DDT-B/2(8En4De) model exceeds Improved-REPA-B/2 by 2.8 FID gains. Our DDT-XL/2(22En6De) exceeds REPA-XL/2 by 1.3 FID gains. While the decoder-only diffusion transformers approach performance saturation with REPA[52], our DDT models continue to deliver superior results. The incremental technique improvements show diminishing gains, particularly in larger model sizes. However, our DDT models maintain a significant performance advantage, underscoring the effectiveness of our approach. Model FID↓↓\\downarrow↓ sFID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Prec.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ SiT-B/2 [32] 33.0 6.46 43.7 0.53 0.63 REPA-B/2 [52] 24.4 6.40 59.9 0.59 0.65 REPA-B/2(Reproduced) 22.2 7.50 69.1 0.59 0.65 DDT-B/2† (8En4De) 21.1 7.81 73.0 0.60 0.65 Improved-SiT-B/2 25.1 6.54 58.8 0.57 0.64 Improved-REPA-B/2 19.1 6.88 76.49 0.60 0.66 DDT-B/2 (8En4De) 16.32 6.63 86.0 0.62 0.66 SiT-L/2 [32] 18.8 5.29 72.0 0.64 0.64 REPA-L/2 [52] 10.0 5.20 109.2 0.69 0.65 Improved-SiT-L/2 12.7 5.48 95.7 0.65 0.65 Improved-REPA-L/2 9.3 5.44 116.6 0.67 0.66 DDT-L/2 (20En4De) 7.98 5.50 128.1 0.68 0.67 SiT-XL/2 [32] 17.2 5.07 76.52 0.65 0.63 REPA-XL/2 [52] 7.9 5.06 122.6 0.70 0.65 Improved-SiT-XL/2 10.9 5.3 103.4 0.66 0.65 Improved-REPA-XL/2 8.14 5.34 124.9 0.68 0.67 DDT-XL/2 (22En6De) 6.62 4.86 135.1 0.69 0.67 Table 2: Metrics of 400⁢K400𝐾400K400 italic_K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. By default, our DDT models are built on improved baselines. DDT† means model built on naive baseline without architecture improvement and lognorm sampling, consistent to REPA. Our DDT models consistently outperformed their counterparts."
  },
  {
    "title": "ImageNet 256×256256256256\\times 256256 × 256.",
    "level": 4,
    "content": "We report the final metrics of DDT-XL/2 (22En6De) and DDT-L/2 (20En4De) at Tab. 1. Our DDT models demonstrate exceptional efficiency, achieving convergence in approximately 1414\\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG of the total epochs compared to REPA [52] and other diffusion transformer models. In order to maintain methodological consistency with REPA, we employed the classifier-free guidance with 2.0 in the interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ], Our models delivered impressive results: DDT-L/2 achieved 1.64 FID, and DDT-XL/2 got 1.52 FID within just 80 epochs. By extending training to 256 epochs—still significantly more efficient than traditional 800-epoch approaches—our DDT-XL/2 established a new state-of-the-art benchmark of 1.31 FID on ImageNet 256×256, decisively outperforming previous diffusion transformer methodologies. To extend training to 400400400400 epochs, our DDT-XL/2(22En6De) achieves 1.26 FID, nearly reaching the upper limit of SD-VAE-ft-EMA-f8d4, which has a 1.20 rFID on ImageNet256256256256."
  },
  {
    "title": "ImageNet 512×512512512512\\times 512512 × 512",
    "level": 4,
    "content": "We provide the final metrics of DDT-XL/2 at Tab. 3. To validate the superiority of our DDT model, we take our DDT-XL/2 trained on ImageNet 256×256256256256\\times 256256 × 256 under 256 epochs as the initialization, fine-tune out DDT-XL/2 on ImageNet 512×512512512512\\times 512512 × 512 for 100⁢K100𝐾100K100 italic_K steps. We adopt the aforementioned interval guidance [26] and we achieved a remarkable state-of-the-art performance of 1.90 FID, decisively outperforming REPA by a significant 0.28 performance margin. In Tab. 3, some metrics exhibit subtle degradation, we attribute this to potentially insufficient fine-tuning. When allocating more training iterations to DDT-XL/2, it achieves 1.281.281.281.28 FID at 500K steps with CFG3.0 within the time interval [0.3,1.0]0.31.0[0.3,1.0][ 0.3 , 1.0 ]. ImageNet 512×512512512512\\times 512512 × 512 Model FID↓↓\\downarrow↓ sFID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Pre.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ BigGAN-deep [3] 8.43 8.13 177.90 0.88 0.29 StyleGAN-XL [40] 2.41 4.06 267.75 0.77 0.52 ADM-G [10] 7.72 6.57 172.71 0.87 0.42 ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53 DiT-XL/2 [36] 3.04 5.02 240.82 0.84 0.54 SiT-XL/2 [32] 2.62 4.18 252.21 0.84 0.57 REPA-XL/2 [52] 2.08 4.19 274.6 0.83 0.58 FlowDCN-XL/2 [48] 2.44 4.53 252.8 0.84 0.54 DDT-XL/2 (500K) 1.28 4.22 305.1 0.80 0.63 Table 3: Benchmarking class-conditional image generation on ImageNet 512×\\times×512. Our DDT-XL/2(512×512512512512\\times 512512 × 512) is fine-tuned from the same model trained on 256×256256256256\\times 256256 × 256 resolution setting of 1.28M steps. We adopt the interval guidance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] and CFG of 3.0"
  },
  {
    "title": "5.4 Acceleration by Encoder sharing",
    "level": 3,
    "content": "As illustrated in Fig. 5, there is a strong local consistency of the self-condition in our condition encoder. Even 𝒛t=0subscript𝒛𝑡0{\\boldsymbol{z}}_{t=0}bold_italic_z start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT has a strong similarity above 0.8 with 𝒛t=1subscript𝒛𝑡1{\\boldsymbol{z}}_{t=1}bold_italic_z start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT. This consistency provides an opportunity to speed up inference by sharing the encoder between adjacent steps. We employed the simple uniform encoder sharing strategy and the new novel statistics dynamic programming strategy. Specifically, for the uniform strategy, we only recalculate the self-condition 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT every K𝐾Kitalic_K steps. For statistics dynamic programming, we solve the aforementioned minimal sum path on the similarity matrix by dynamic programming and recalculate 𝒛tsubscript𝒛𝑡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT according to the solved strategy. As shown in Fig. 6, there is a significant inference speedup nearly without visual quality loss when K𝐾Kitalic_K is smaller than 6. As shown in Tab. 4, the metrics loss is still marginal, while the inference speedup is significant. The novel statistics dynamic programming slightly outperformed the naive uniform strategy with less FID drop. SharRatio Acc ΦΦ\\Phiroman_Φ FID↓↓\\downarrow↓ sFID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Prec.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ 0.00 1.0×1.0\\times1.0 × Uniform 1.31 4.62 308.1 0.78 0.66 0.50 1.6×1.6\\times1.6 × Uniform 1.31 4.48 300.5 0.78 0.65 0.66 1.9×1.9\\times1.9 × Uniform 1.32 4.46 301.2 0.78 0.65 0.75 2.3×2.3\\times2.3 × Uniform 1.34 4.43 302.7 0.78 0.65 0.80 2.6×2.6\\times2.6 × Uniform 1.36 4.40 303.3 0.78 0.64 StatisticDP 1.33 4.37 301.7 0.78 0.64 0.83 2.7×2.7\\times2.7 × Uniform 1.37 4.41 302.8 0.78 0.64 StatisticDP 1.36 4.35 300.3 0.78 0.64 0.87 3.0×3.0\\times3.0 × Uniform 1.42 4.43 302.8 0.78 0.64 StatisticDP 1.40 4.35 302.4 0.78 0.64 Table 4: Metrics of 400⁢K400𝐾400K400 italic_K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. Our DDT models consistently outperformed its counterparts Figure 5: The cosine similarity of self-condition feature ztsubscript𝑧𝑡\\boldsymbol{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from encoder between different timesteps. There is a strong correlation between adjacent steps, indicating the redundancy. Figure 6: Sharing the self-condition ztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in adjacent steps significant speedup the inference.We tried various sharing frequency configurations. There is marginal visual quality down-gradation when the sharing frequency is reasonable."
  },
  {
    "title": "5.5 Ablations",
    "level": 3,
    "content": "(a) (b) (c) Figure 7: The DDT-B/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-B/2(8En4De) achieves much faster convergence speed and better performance. (a) (b) (c) Figure 8: The DDT-L/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-L/2 prefers an unexpected aggressive encoder-deocder ratio DDT-L/2(20En4De) achieves much faster convergence speed and better performance. We conduct ablation studies on ImageNet 256×256256256256\\times 256256 × 256 with DDT-B/2 and DDT-L/2. For sampling, we take the Euler solver with 250 steps as the default choice without classifier-free guidance. For training, we train each model with 80 epochs(400k steps), and the batch size is set to 256."
  },
  {
    "title": "Encoder-Decoder Ratio",
    "level": 4,
    "content": "we systematically explored ratios ranging from 2:1:212:12 : 1 to 5:1:515:15 : 1 across different model sizes. in Fig. 7 and Fig. 8. Our notation m𝑚mitalic_mEnn𝑛nitalic_nDe represents models with m𝑚mitalic_m encoder layers and n𝑛nitalic_n decoder layers. The investigation experiments in Fig. 7 and Fig. 8 revealed critical insights into architectural optimization. We observed that a larger encoder is beneficial for further improving the performance as the model size increases. For the Base model in Fig. 7, the optimal configuration emerged as 8 encoder layers and 4 decoder layers, delivering superior performance and convergence speed. Notably, the Large model in Fig. 8 exhibited a distinct preference, achieving peak performance with 20 encoder layers and 4 decoder layers, an unexpectedly aggressive encoder-decoder ratio. This unexpected discovery motivates us to scale the layer ratio in DDT-XL/2 to 22 encoder layers and 6 decoders to explore the performance upper limits of diffusion transformers."
  },
  {
    "title": "Decoder Block types.",
    "level": 4,
    "content": "In our investigation of decoder block types and their impact on high-frequency decoding performance, we systematically evaluated multiple architectural configurations. Our comprehensive assessment included alternative approaches such as simple 3×3 convolution blocks and naive MLP blocks. As shown in Tab. 5, the default (Attention with the MLP) setting achieves better results. Thanks to the encoder-decoder design, naive Conv blocks even achieve comparable results. DecoderBlock FID↓↓\\downarrow↓ sFID↓↓\\downarrow↓ IS↑↑\\uparrow↑ Prec.↑↑\\uparrow↑ Rec.↑↑\\uparrow↑ Conv+MLP 16.96 7.33 85.1 0.62 0.65 MLP+MLP 24.13 7.89 65.0 0.57 0.65 Attn+MLP 16.32 6.63 86.0 0.62 0.66 Table 5: Metrics of 400⁢K400𝐾400K400 italic_K training steps on DDT-B/2(8En4De) with different decoder blocks. All results are reported without classifier-free guidance. The Default Attention + MLP configuration achieves best performance."
  },
  {
    "title": "6 Conclusion",
    "level": 2,
    "content": "In this paper, we have introduced a novel Decoupled Diffusion Transformer, which rethinks the optimization dilemma of the traditional diffusion transformer. By decoupling the low-frequency encoding and high-frequency decoding into dedicated components, we effectively resolved the optimization dilemma that has constrained diffusion transformer. Furthermore, we discovered that increasing the encoder capacity relative to the decoder yields increasingly beneficial results as the overall model scale grows. This insight provides valuable guidance for future model scaling efforts. Our experiments demonstrate that our DDT-XL/2 (22En6De) with an unexpected aggressive encoder-decoder layer ratio achieves great performance while requiring only 256 training epochs. This significant improvement in efficiency addresses one of the primary limitations of diffusion models: their lengthy training requirements. The decoupled architecture also presents opportunities for inference optimization through our proposed encoder result sharing mechanism. Our statistical dynamic programming approach for determining optimal sharing strategies enables faster inference while minimizing quality degradation, demonstrating that architectural innovations can yield benefits beyond their primary design objectives."
  },
  {
    "title": "References",
    "level": 2,
    "content": "Agarwal et al. [2025] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Bao et al. [2023] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669–22679, 2023. Brock et al. [2018] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213–229. Springer, 2020. Chang et al. [2022] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315–11325, 2022. Chen et al. [2023] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\\\\\backslash\\alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen et al. [2024] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\\\\\\backslash\\sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. Chu et al. [2024] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: A unified llama interface for vision tasks. arXiv preprint arXiv:2403.00522, 2024. Deng et al. [2024] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021. Dieleman [2024] Sander Dieleman. Diffusion is spectral autoregression, 2024. Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Fei et al. [2024] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. Gao et al. [2023a] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23164–23173, 2023a. Gao et al. [2023b] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164–23173, 2023b. Hang et al. [2023] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7441–7451, 2023. He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000–16009, 2022. Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. Hong et al. [2022] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022. Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015–4026, 2023. Kong et al. [2024] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kynkäänniemi et al. [2019] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Kynkäänniemi et al. [2024] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in a limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Li et al. [2024] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: A self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441–125468, 2024. Li et al. [2025] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:56424–56445, 2025. Lipman et al. [2022] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu et al. [2022] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Lu et al. [2024] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024. Ma et al. [2024a] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024a. Ma et al. [2024b] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15762–15772, 2024b. Nash et al. [2021] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. Oquab et al. [2023] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023. Rissanen et al. [2022] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022. Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sauer et al. [2022] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1–10, 2022. Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Su et al. [2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sun et al. [2024] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Teng et al. [2023] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Teng et al. [2024] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Wang et al. [2024] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcn-like architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024. Yan et al. [2023] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. Yao and Wang [2025] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Yu et al. [2024a] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024a. Yu et al. [2024b] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. Yue et al. [2024] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. Zhuo et al. [2024] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024."
  },
  {
    "title": "Appendix A Model Specs",
    "level": 2,
    "content": "Config #Layers Hidden dim #Heads B/2 12 768 12 L/2 24 1024 16 XL/2 28 1152 16"
  },
  {
    "title": "Appendix B Hyper-parameters",
    "level": 2,
    "content": "VAE SD-VAE-f8d4-ft-ema VAE donwsample 8 latent channel 4 optimizer AdamW [22] base learning rate 1e-4 weight decay 0.0 batch size 256 learning rate schedule constant augmentation center crop diffusion sampler Euler-ODE diffusion steps 250 evaluation suite ADM [10]"
  },
  {
    "title": "Appendix C Linear flow and Diffusion",
    "level": 2,
    "content": "Given the SDE forward and reverse process: d⁢𝒙t𝑑subscript𝒙𝑡\\displaystyle{d}{\\boldsymbol{x}}_{t}italic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =f⁢(t)⁢𝒙t⁢d⁢t+g⁢(t)⁢d⁢𝒘absent𝑓𝑡subscript𝒙𝑡d𝑡𝑔𝑡d𝒘\\displaystyle=f(t){\\boldsymbol{x}}_{t}\\mathrm{d}t+g(t)\\mathrm{d}{\\boldsymbol{w}}= italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_d italic_t + italic_g ( italic_t ) roman_d bold_italic_w (9) d⁢𝒙t𝑑subscript𝒙𝑡\\displaystyle{d}{\\boldsymbol{x}}_{t}italic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =[f⁢(t)⁢𝒙t−g⁢(t)2⁢∇𝒙log⁡p⁢(𝒙t)]⁢d⁢t+g⁢(t)⁢d⁢𝒘absentdelimited-[]𝑓𝑡subscript𝒙𝑡𝑔superscript𝑡2subscript∇𝒙𝑝subscript𝒙𝑡𝑑𝑡𝑔𝑡𝑑𝒘\\displaystyle=[f(t){\\boldsymbol{x}}_{t}-g(t)^{2}\\nabla_{\\boldsymbol{x}}\\log p(% {\\boldsymbol{x}}_{t})]dt+g(t){d}{\\boldsymbol{w}}= [ italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] italic_d italic_t + italic_g ( italic_t ) italic_d bold_italic_w (10) A corresponding deterministic process exists with trajectories sharing the same marginal probability densities of reverse SDE. d⁢𝒙t=[f⁢(t)⁢𝒙t−12⁢g⁢(t)2⁢∇𝒙tlog⁡p⁢(𝒙t)]⁢d⁢t𝑑subscript𝒙𝑡delimited-[]𝑓𝑡subscript𝒙𝑡12𝑔superscript𝑡2subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡𝑑𝑡{d}{\\boldsymbol{x}}_{t}=[f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{% \\boldsymbol{x}_{t}}\\log p({\\boldsymbol{x}}_{t})]{d}titalic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = [ italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] italic_d italic_t (11) Given xt=αt⁢xd⁢a⁢t⁢a+σ⁢ϵsubscript𝑥𝑡subscript𝛼𝑡subscript𝑥𝑑𝑎𝑡𝑎𝜎italic-ϵx_{t}=\\alpha_{t}x_{data}+\\sigma\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_σ italic_ϵ. The traditional diffusion model learns: ∇𝒙tlog⁡p⁢(𝒙t)=−ϵσ⁢(t)subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡italic-ϵ𝜎𝑡\\nabla_{\\boldsymbol{x}_{t}}\\log p({\\boldsymbol{x}}_{t})=-{\\frac{\\epsilon}{% \\sigma(t)}}∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - divide start_ARG italic_ϵ end_ARG start_ARG italic_σ ( italic_t ) end_ARG (12) The flow-matching framework actually learns the following: 𝒗tsubscript𝒗𝑡\\displaystyle{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =α˙⁢x+σ˙⁢ϵabsent˙𝛼𝑥˙𝜎italic-ϵ\\displaystyle=\\dot{\\alpha}x+\\dot{\\sigma}\\epsilon= over˙ start_ARG italic_α end_ARG italic_x + over˙ start_ARG italic_σ end_ARG italic_ϵ (13) =x−ϵabsent𝑥italic-ϵ\\displaystyle=x-\\epsilon= italic_x - italic_ϵ (14) Here we will demonstrate in flow-matching, the 𝒗tsubscript𝒗𝑡{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT prediction is actually as same as the reverse ode: α˙⁢x+σ˙⁢ϵ˙𝛼𝑥˙𝜎italic-ϵ\\displaystyle\\dot{\\alpha}x+\\dot{\\sigma}\\epsilonover˙ start_ARG italic_α end_ARG italic_x + over˙ start_ARG italic_σ end_ARG italic_ϵ (15) =\\displaystyle== f⁢(t)⁢𝒙t−12⁢g⁢(t)2⁢∇𝒙tlog⁡p⁢(𝒙t)𝑓𝑡subscript𝒙𝑡12𝑔superscript𝑡2subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡\\displaystyle f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{\\boldsymbol{% x}_{t}}\\log p({\\boldsymbol{x}}_{t})italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (16) Let us start by expanding the reverse ode first. f⁢(t)⁢𝒙t−12⁢g⁢(t)2⁢∇𝒙tlog⁡p⁢(𝒙t)𝑓𝑡subscript𝒙𝑡12𝑔superscript𝑡2subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡\\displaystyle f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{\\boldsymbol{% x}_{t}}\\log p({\\boldsymbol{x}}_{t})italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (17) =\\displaystyle== f⁢(t)⁢(α⁢(t)⁢𝒙d⁢a⁢t⁢a+σ⁢(t)⁢ϵ)−12⁢g⁢(t)2⁢[−ϵσ⁢(t)]𝑓𝑡𝛼𝑡subscript𝒙𝑑𝑎𝑡𝑎𝜎𝑡italic-ϵ12𝑔superscript𝑡2delimited-[]italic-ϵ𝜎𝑡\\displaystyle f(t)(\\alpha(t){\\boldsymbol{x}}_{data}+\\sigma(t)\\epsilon)-\\frac{1% }{2}g(t)^{2}[{-\\frac{\\epsilon}{\\sigma(t)}}]italic_f ( italic_t ) ( italic_α ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_σ ( italic_t ) italic_ϵ ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ - divide start_ARG italic_ϵ end_ARG start_ARG italic_σ ( italic_t ) end_ARG ] (18) =\\displaystyle== f⁢(t)⁢α⁢(t)⁢𝒙d⁢a⁢t⁢a+(f⁢(t)⁢σ⁢(t)+12⁢g⁢(t)2σ⁢(t))⁢ϵ𝑓𝑡𝛼𝑡subscript𝒙𝑑𝑎𝑡𝑎𝑓𝑡𝜎𝑡12𝑔superscript𝑡2𝜎𝑡italic-ϵ\\displaystyle f(t)\\alpha(t){\\boldsymbol{x}}_{data}+(f(t)\\sigma(t)+\\frac{1}{2}{% \\frac{g(t)^{2}}{\\sigma(t)}})\\epsilonitalic_f ( italic_t ) italic_α ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + ( italic_f ( italic_t ) italic_σ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_σ ( italic_t ) end_ARG ) italic_ϵ (19) To prove Eq. 16, we needs to demonstrate that: α˙⁢(t)˙𝛼𝑡\\displaystyle\\dot{\\alpha}(t)over˙ start_ARG italic_α end_ARG ( italic_t ) =ft⁢α⁢(t)absentsubscript𝑓𝑡𝛼𝑡\\displaystyle=f_{t}\\alpha(t)= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) (20) σ˙⁢(t)˙𝜎𝑡\\displaystyle\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) =ft⁢σ⁢(t)+12⁢gt2σ⁢(t).absentsubscript𝑓𝑡𝜎𝑡12superscriptsubscript𝑔𝑡2𝜎𝑡\\displaystyle=f_{t}\\sigma(t)+\\frac{1}{2}\\frac{{g_{t}^{2}}}{\\sigma(t)}.= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_σ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_σ ( italic_t ) end_ARG . (21) Here, let us derive the relation between ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and α⁢(t),α˙⁢(t)𝛼𝑡˙𝛼𝑡\\alpha(t),\\dot{\\alpha}(t)italic_α ( italic_t ) , over˙ start_ARG italic_α end_ARG ( italic_t ). We donate xd⁢a⁢t⁢a⁢(t)=α⁢(t)⁢xd⁢a⁢t⁢asubscript𝑥𝑑𝑎𝑡𝑎𝑡𝛼𝑡subscript𝑥𝑑𝑎𝑡𝑎x_{data}(t)=\\alpha(t)x_{data}italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) = italic_α ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT is the remain component of xd⁢a⁢t⁢asubscript𝑥𝑑𝑎𝑡𝑎x_{data}italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT in xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, it is easy to find that: d⁢𝒙d⁢a⁢t⁢a⁢(t)𝑑subscript𝒙𝑑𝑎𝑡𝑎𝑡\\displaystyle d{\\boldsymbol{x}}_{data}(t)italic_d bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) =ft⁢𝒙d⁢a⁢t⁢a⁢(t)⁢d⁢tabsentsubscript𝑓𝑡subscript𝒙𝑑𝑎𝑡𝑎𝑡𝑑𝑡\\displaystyle=f_{t}{\\boldsymbol{x}}_{data}(t)dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) italic_d italic_t (22) d⁢(α⁢(t)⁢xd⁢a⁢t⁢a)𝑑𝛼𝑡subscript𝑥𝑑𝑎𝑡𝑎\\displaystyle d(\\alpha(t)x_{data})italic_d ( italic_α ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ) =ft⁢α⁢(t)⁢xd⁢a⁢t⁢a⁢d⁢tabsentsubscript𝑓𝑡𝛼𝑡subscript𝑥𝑑𝑎𝑡𝑎𝑑𝑡\\displaystyle=f_{t}\\alpha(t)x_{data}dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT italic_d italic_t (23) d⁢α⁢(t)𝑑𝛼𝑡\\displaystyle d\\alpha(t)italic_d italic_α ( italic_t ) =ft⁢α⁢(t)⁢d⁢tabsentsubscript𝑓𝑡𝛼𝑡𝑑𝑡\\displaystyle=f_{t}\\alpha(t)dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) italic_d italic_t (24) So, Eq. 20 is right. Based on the above equation, we will demonstrate the relation of gt,ftsubscript𝑔𝑡subscript𝑓𝑡g_{t},f_{t}italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with σ⁢(t)𝜎𝑡\\sigma(t)italic_σ ( italic_t ). Note that Gaussian noise has nice additive properties. a⁢ϵ1+b⁢ϵ2∈𝒩⁢(0,a2+b2)𝑎subscriptitalic-ϵ1𝑏subscriptitalic-ϵ2𝒩0superscript𝑎2superscript𝑏2a\\epsilon_{1}+b\\epsilon_{2}\\in\\mathcal{N}(0,\\sqrt{a^{2}+b^{2}})italic_a italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ caligraphic_N ( 0 , square-root start_ARG italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_b start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) (25) Let us start with the gaussian noise component ϵ⁢(t)italic-ϵ𝑡\\epsilon(t)italic_ϵ ( italic_t ) calculation, reaching at t𝑡titalic_t, every noise addition at s∈[0,t]𝑠0𝑡s\\in[0,t]italic_s ∈ [ 0 , italic_t ] while been decayed by a factor of α⁢(t)α⁢(s)𝛼𝑡𝛼𝑠\\frac{\\alpha(t)}{\\alpha(s)}divide start_ARG italic_α ( italic_t ) end_ARG start_ARG italic_α ( italic_s ) end_ARG. Thus, the mixed Gaussian noise will have a std variance σ⁢(t)𝜎𝑡\\sigma(t)italic_σ ( italic_t ) of: σ⁢(t)𝜎𝑡\\displaystyle\\sigma(t)italic_σ ( italic_t ) =(∫0t[(α⁢(t)α⁢(s))2⁢gs2]⁢𝑑s)absentsuperscriptsubscript0𝑡delimited-[]superscript𝛼𝑡𝛼𝑠2subscriptsuperscript𝑔2𝑠differential-d𝑠\\displaystyle=\\sqrt{(\\int_{0}^{t}[(\\frac{\\alpha(t)}{\\alpha(s)})^{2}g^{2}_{s}]% ds)}= square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_α ( italic_t ) end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ] italic_d italic_s ) end_ARG (26) σ⁢(t)𝜎𝑡\\displaystyle\\sigma(t)italic_σ ( italic_t ) =α⁢(t)⁢(∫0t[(gsα⁢(s))2]⁢𝑑s)absent𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑠𝛼𝑠2differential-d𝑠\\displaystyle=\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{s}}{\\alpha(s)})^{2}]ds)}= italic_α ( italic_t ) square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG (27) After obtaining the relation of ft,gtsubscript𝑓𝑡subscript𝑔𝑡{f_{t},g_{t}}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and α⁢(t),σ⁢(t)𝛼𝑡𝜎𝑡{\\alpha(t),\\sigma(t)}italic_α ( italic_t ) , italic_σ ( italic_t ), we derive α˙⁢(t)˙𝛼𝑡\\dot{\\alpha}(t)over˙ start_ARG italic_α end_ARG ( italic_t ) and σ˙⁢(t)˙𝜎𝑡\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) with above conditions: α˙⁢(t)˙𝛼𝑡\\displaystyle\\dot{\\alpha}(t)over˙ start_ARG italic_α end_ARG ( italic_t ) =ft⁢exp⁡[∫0tfs⁢𝑑s]absentsubscript𝑓𝑡subscriptsuperscript𝑡0subscript𝑓𝑠differential-d𝑠\\displaystyle=f_{t}\\exp[\\int^{t}_{0}f_{s}ds]= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_exp [ ∫ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_d italic_s ] (28) α˙⁢(t)˙𝛼𝑡\\displaystyle\\dot{\\alpha}(t)over˙ start_ARG italic_α end_ARG ( italic_t ) =ft⁢α⁢(t)absentsubscript𝑓𝑡𝛼𝑡\\displaystyle=f_{t}\\alpha(t)= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) (29) As for σ˙⁢(t)˙𝜎𝑡\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ), it is quit complex but not hard: σ˙⁢(t)˙𝜎𝑡\\displaystyle\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) =α˙⁢(t)⁢(∫0t[(gtα⁢(s))2]⁢𝑑s)+α⁢(t)⁢12⁢gt2α⁢(t)(∫0t[(gtα⁢(s))2⁢gs2]⁢𝑑s)absent˙𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2differential-d𝑠𝛼𝑡12superscriptsubscript𝑔𝑡2𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2subscriptsuperscript𝑔2𝑠differential-d𝑠\\displaystyle=\\dot{\\alpha}(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}% ]ds)}+\\alpha(t)\\frac{\\frac{1}{2}\\frac{g_{t}^{2}}{\\alpha(t)}}{\\sqrt{(\\int_{0}^{% t}[(\\frac{g_{t}}{\\alpha(s)})^{2}g^{2}_{s}]ds)}}= over˙ start_ARG italic_α end_ARG ( italic_t ) square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + italic_α ( italic_t ) divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α ( italic_t ) end_ARG end_ARG start_ARG square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ] italic_d italic_s ) end_ARG end_ARG (30) σ˙⁢(t)˙𝜎𝑡\\displaystyle\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) =(ft⁢α⁢(t))⁢(∫0t[(gtα⁢(s))2]⁢𝑑s)+α⁢(t)⁢12⁢gt2α2⁢(t)(∫0t[(gtα⁢(s))2]⁢𝑑s)absentsubscript𝑓𝑡𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2differential-d𝑠𝛼𝑡12subscriptsuperscript𝑔2𝑡superscript𝛼2𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2differential-d𝑠\\displaystyle=(f_{t}\\alpha(t))\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2% }]ds)}+\\alpha(t)\\frac{\\frac{1}{2}\\frac{g^{2}_{t}}{\\alpha^{2}(t)}}{\\sqrt{(\\int_% {0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}]ds)}}= ( italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) ) square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + italic_α ( italic_t ) divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_t ) end_ARG end_ARG start_ARG square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG end_ARG (31) σ˙⁢(t)˙𝜎𝑡\\displaystyle\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) =ft⁢α⁢(t)⁢(∫0t[(gtα⁢(s))2]⁢𝑑s)+12⁢gt2α⁢(t)⁢(∫0t[(gtα⁢(s))2]⁢𝑑s)absentsubscript𝑓𝑡𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2differential-d𝑠12superscriptsubscript𝑔𝑡2𝛼𝑡superscriptsubscript0𝑡delimited-[]superscriptsubscript𝑔𝑡𝛼𝑠2differential-d𝑠\\displaystyle=f_{t}\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}]% ds)}+\\frac{\\frac{1}{2}g_{t}^{2}}{\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{% \\alpha(s)})^{2}]ds)}}= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α ( italic_t ) square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α ( italic_t ) square-root start_ARG ( ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_α ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG end_ARG (32) σ˙⁢(t)˙𝜎𝑡\\displaystyle\\dot{\\sigma}(t)over˙ start_ARG italic_σ end_ARG ( italic_t ) =ft⁢σ⁢(t)+12⁢g⁢tσ⁢(t)absentsubscript𝑓𝑡𝜎𝑡12𝑔𝑡𝜎𝑡\\displaystyle=f_{t}\\sigma(t)+\\frac{1}{2}\\frac{gt}{\\sigma(t)}= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_σ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g italic_t end_ARG start_ARG italic_σ ( italic_t ) end_ARG (33) So, Eq. 21 is right."
  },
  {
    "title": "Appendix D Proof of Spectrum Autoregressive",
    "level": 2,
    "content": "Given the noise scheduler{αt,σt}subscript𝛼𝑡subscript𝜎𝑡\\{\\alpha_{t},\\sigma_{t}\\}{ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }, the clean data 𝒙datasubscript𝒙data{\\boldsymbol{x}}_{\\text{data}}bold_italic_x start_POSTSUBSCRIPT data end_POSTSUBSCRIPT and Gaussian noise ϵitalic-ϵ\\epsilonitalic_ϵ. Denote Kf⁢r⁢e⁢qsubscript𝐾𝑓𝑟𝑒𝑞K_{freq}italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT as the maximum frequency of the clean data 𝒙d⁢a⁢t⁢asubscript𝒙𝑑𝑎𝑡𝑎{\\boldsymbol{x}}_{data}bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT The noisy latent xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestep t𝑡titalic_t has been defined as: 𝒙t=αt⁢𝒙d⁢a⁢t⁢a+σt⁢ϵsubscript𝒙𝑡subscript𝛼𝑡subscript𝒙𝑑𝑎𝑡𝑎subscript𝜎𝑡bold-italic-ϵ{\\boldsymbol{x}}_{t}=\\alpha_{t}{\\boldsymbol{x}}_{data}+\\sigma_{t}{\\boldsymbol{% \\epsilon}}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_ϵ (34) The spectrum magnitude 𝒄isubscript𝒄𝑖{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTof xtsubscript𝑥𝑡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT on DCT basics 𝒖isubscript𝒖𝑖{\\boldsymbol{u}}_{i}bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT follows: 𝒄isubscript𝒄𝑖\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =𝔼ϵ⁢[𝒖iT⁢𝒙t]2absentsubscript𝔼italic-ϵsuperscriptdelimited-[]superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑡2\\displaystyle=\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{% t}]^{2}= blackboard_E start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 𝒄isubscript𝒄𝑖\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =𝔼ϵ⁢[𝒖iT⁢(αt⁢𝒙d⁢a⁢t⁢a+σt⁢ϵ)]2absentsubscript𝔼italic-ϵsuperscriptdelimited-[]superscriptsubscript𝒖𝑖𝑇subscript𝛼𝑡subscript𝒙𝑑𝑎𝑡𝑎subscript𝜎𝑡bold-italic-ϵ2\\displaystyle=\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}(\\alpha_{t}{% \\boldsymbol{x}}_{data}+\\sigma_{t}{\\boldsymbol{\\epsilon}})]^{2}= blackboard_E start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_ϵ ) ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Recall that the spectrum magnitude of Gaussian noise ϵitalic-ϵ\\epsilonitalic_ϵ is uniformly distributed. 𝒄isubscript𝒄𝑖\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =[αt⁢𝒖iT⁢𝒙d⁢a⁢t⁢a]2+2⁢αt⁢σt⁢𝔼ϵ⁢[𝒖iT⁢𝒙d⁢a⁢t⁢a⁢𝒖iT⁢ϵ]+σt2⁢𝔼ϵ⁢[𝒖iT⁢ϵ]2absentsuperscriptdelimited-[]subscript𝛼𝑡superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎22subscript𝛼𝑡subscript𝜎𝑡subscript𝔼italic-ϵdelimited-[]superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎superscriptsubscript𝒖𝑖𝑇italic-ϵsuperscriptsubscript𝜎𝑡2subscript𝔼bold-italic-ϵsuperscriptdelimited-[]superscriptsubscript𝒖𝑖𝑇italic-ϵ2\\displaystyle=[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}+% 2\\alpha_{t}\\sigma_{t}\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}{% \\boldsymbol{x}}_{data}{\\boldsymbol{u}}_{i}^{T}\\epsilon]+\\sigma_{t}^{2}\\mathbb{% E}_{\\boldsymbol{\\epsilon}}[{\\boldsymbol{u}}_{i}^{T}\\epsilon]^{2}= [ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ϵ ] + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ϵ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 𝒄isubscript𝒄𝑖\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =[αt⁢𝒖iT⁢𝒙d⁢a⁢t⁢a]2+σt2⁢𝔼ϵ⁢[𝒖iT⁢ϵ]2absentsuperscriptdelimited-[]subscript𝛼𝑡superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎2superscriptsubscript𝜎𝑡2subscript𝔼bold-italic-ϵsuperscriptdelimited-[]superscriptsubscript𝒖𝑖𝑇italic-ϵ2\\displaystyle=[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}+% \\sigma_{t}^{2}\\mathbb{E}_{\\boldsymbol{\\epsilon}}[{\\boldsymbol{u}}_{i}^{T}% \\epsilon]^{2}= [ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_ϵ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ϵ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 𝒄isubscript𝒄𝑖\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =αt2⁢[𝒖iT⁢𝒙d⁢a⁢t⁢a]2+σt2⁢λabsentsuperscriptsubscript𝛼𝑡2superscriptdelimited-[]superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎2superscriptsubscript𝜎𝑡2𝜆\\displaystyle=\\alpha_{t}^{2}[{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^% {2}+\\sigma_{t}^{2}\\lambda= italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_λ if σt2⁢λsuperscriptsubscript𝜎𝑡2𝜆\\sigma_{t}^{2}\\lambdaitalic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_λ has bigger value than [αt⁢𝒖iT⁢𝒙d⁢a⁢t⁢a]2superscriptdelimited-[]subscript𝛼𝑡superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎2[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}[ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, the spectrum magnitude 𝒄isubscript𝒄𝑖{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT on DCT basics 𝒖isubscript𝒖𝑖{\\boldsymbol{u}}_{i}bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT will be canceled, thus the maximal remaining frequency fm⁢a⁢x⁢(t)subscript𝑓𝑚𝑎𝑥𝑡f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) of original data in 𝒙tsubscript𝒙𝑡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows: fm⁢a⁢x⁢(t)>min⁡((αt⁢𝒖iT⁢𝒙d⁢a⁢t⁢aσt⁢λ)2,Kf⁢r⁢e⁢q)subscript𝑓𝑚𝑎𝑥𝑡superscriptsubscript𝛼𝑡superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎subscript𝜎𝑡𝜆2subscript𝐾𝑓𝑟𝑒𝑞f_{max}(t)>\\min\\left({\\left(\\frac{\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{% \\boldsymbol{x}}_{data}}{\\sigma_{t}\\lambda}\\right)}^{2},K_{freq}\\right)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_λ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) (35) Though αt⁢𝒖iT⁢𝒙d⁢a⁢t⁢aσt⁢λ2superscriptsubscript𝛼𝑡superscriptsubscript𝒖𝑖𝑇subscript𝒙𝑑𝑎𝑡𝑎subscript𝜎𝑡𝜆2{\\frac{\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}}{\\sigma_{t}% \\lambda}}^{2}divide start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_λ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT depends on the dataset. Here, we directly suppose it as a constant 1111. And replace α=t𝛼𝑡\\alpha=titalic_α = italic_t and σ=1−t𝜎1𝑡\\sigma=1-titalic_σ = 1 - italic_t in above equation: fm⁢a⁢x⁢(t)>min⁡((t1−t)2,Kf⁢r⁢e⁢q)subscript𝑓𝑚𝑎𝑥𝑡superscript𝑡1𝑡2subscript𝐾𝑓𝑟𝑒𝑞f_{max}(t)>\\min\\left({\\left(\\frac{t}{1-t}\\right)}^{2},K_{freq}\\right)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_t end_ARG start_ARG 1 - italic_t end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) (36)"
  },
  {
    "title": "Appendix E Linear multisteps method",
    "level": 2,
    "content": "We conduct targeted experiment on SiT-XL/2 with Adams–Bashforth like linear multistep solver; To clarify, we did not employ this powerful solver for our DDT models in all tables across the main paper. The reverse ode of the diffusion models tackles the following integral: 𝒙i+1=𝒙i+∫titi+1𝒗θ⁢(𝒙t,t)⁢𝑑tsubscript𝒙𝑖1subscript𝒙𝑖superscriptsubscriptsubscript𝑡𝑖subscript𝑡𝑖1subscript𝒗𝜃subscript𝒙𝑡𝑡differential-d𝑡{\\boldsymbol{x}}_{i+1}={\\boldsymbol{x}}_{i}+\\int_{t_{i}}^{t_{i+1}}{\\boldsymbol% {v}}_{\\theta}({\\boldsymbol{x}}_{t},t){dt}\\\\ bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ∫ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) italic_d italic_t (37) The classic Euler method employs 𝒗θ⁢(𝒙i,ti)subscript𝒗𝜃subscript𝒙𝑖subscript𝑡𝑖{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{i},t_{i})bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) as an estimate of 𝒗θ⁢(𝒙t,t)subscript𝒗𝜃subscript𝒙𝑡𝑡{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{t},t)bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) throughout the interval [ti,ti+1]subscript𝑡𝑖subscript𝑡𝑖1[t_{i},t_{i+1}][ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ] 𝒙i+1=𝒙i+(ti+1−ti)⁢𝒗θ⁢(𝒙i,ti).subscript𝒙𝑖1subscript𝒙𝑖subscript𝑡𝑖1subscript𝑡𝑖subscript𝒗𝜃subscript𝒙𝑖subscript𝑡𝑖{\\boldsymbol{x}}_{i+1}={\\boldsymbol{x}}_{i}+(t_{i+1}-t_{i}){\\boldsymbol{v}}_{% \\theta}({\\boldsymbol{x}}_{i},t_{i}).\\\\ bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ( italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . (38) The most classic multi-step solver Adams–Bashforth method (deemed as Adams for brevity) incorporates the Lagrange polynomial to improve the estimation accuracy with previous predictions. 𝒗θ⁢(𝒙t,t)subscript𝒗𝜃subscript𝒙𝑡𝑡\\displaystyle{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{t},t)bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) =∑j=0i(∏k=0,k≠jit−tktj−tk)⁢𝒗θ⁢(𝒙j,tj)absentsuperscriptsubscript𝑗0𝑖superscriptsubscriptproductformulae-sequence𝑘0𝑘𝑗𝑖𝑡subscript𝑡𝑘subscript𝑡𝑗subscript𝑡𝑘subscript𝒗𝜃subscript𝒙𝑗subscript𝑡𝑗\\displaystyle=\\sum_{j=0}^{i}(\\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k% }}}){\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{j},t_{j})= ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( ∏ start_POSTSUBSCRIPT italic_k = 0 , italic_k ≠ italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) 𝒙i+1subscript𝒙𝑖1\\displaystyle{\\boldsymbol{x}}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ≈𝒙i+∫titi+1∑j=0i(∏k=0,k≠jit−tktj−tk)⁢𝒗θ⁢(𝒙j,tj)⁢d⁢tabsentsubscript𝒙𝑖superscriptsubscriptsubscript𝑡𝑖subscript𝑡𝑖1superscriptsubscript𝑗0𝑖superscriptsubscriptproductformulae-sequence𝑘0𝑘𝑗𝑖𝑡subscript𝑡𝑘subscript𝑡𝑗subscript𝑡𝑘subscript𝒗𝜃subscript𝒙𝑗subscript𝑡𝑗𝑑𝑡\\displaystyle\\approx{\\boldsymbol{x}}_{i}+\\int_{t_{i}}^{t_{i+1}}\\sum_{j=0}^{i}(% \\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k}}}){\\boldsymbol{v}}_{\\theta}% ({\\boldsymbol{x}}_{j},t_{j})dt≈ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ∫ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( ∏ start_POSTSUBSCRIPT italic_k = 0 , italic_k ≠ italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) italic_d italic_t 𝒙i+1subscript𝒙𝑖1\\displaystyle{\\boldsymbol{x}}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ≈𝒙i+∑j=0i𝒗θ⁢(𝒙j,tj)⁢∫titi+1(∏k=0,k≠jit−tktj−tk)⁢𝑑tabsentsubscript𝒙𝑖superscriptsubscript𝑗0𝑖subscript𝒗𝜃subscript𝒙𝑗subscript𝑡𝑗superscriptsubscriptsubscript𝑡𝑖subscript𝑡𝑖1superscriptsubscriptproductformulae-sequence𝑘0𝑘𝑗𝑖𝑡subscript𝑡𝑘subscript𝑡𝑗subscript𝑡𝑘differential-d𝑡\\displaystyle\\approx{\\boldsymbol{x}}_{i}+\\sum_{j=0}^{i}{\\boldsymbol{v}}_{% \\theta}({\\boldsymbol{x}}_{j},t_{j})\\int_{t_{i}}^{t_{i+1}}(\\prod_{k=0,k\\neq j}^% {i}{\\frac{t-t_{k}}{t_{j}-t_{k}}})dt≈ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ∫ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( ∏ start_POSTSUBSCRIPT italic_k = 0 , italic_k ≠ italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) italic_d italic_t Note that ∫titi+1(∏k=0,k≠jit−tktj−tk)⁢𝑑tsuperscriptsubscriptsubscript𝑡𝑖subscript𝑡𝑖1superscriptsubscriptproductformulae-sequence𝑘0𝑘𝑗𝑖𝑡subscript𝑡𝑘subscript𝑡𝑗subscript𝑡𝑘differential-d𝑡\\int_{t_{i}}^{t_{i+1}}(\\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k}}})dt∫ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( ∏ start_POSTSUBSCRIPT italic_k = 0 , italic_k ≠ italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) italic_d italic_t of the Lagrange polynomial can be pre-integrated into a constant coefficient, resulting in only naive summation being required for ODE solving."
  },
  {
    "title": "Appendix F Classifier free guidance.",
    "level": 2,
    "content": "As classifier-free guidance significantly impacts the performance of diffusion models. Traditional classifier-free guidance improves performance at the cost of decreased diversity. Interval guidance is recently been adopted by REPA[52] and Causalfusion[9], It applies classifier-free guidance only to the high-frequency generation phase to preserve the diversity. We sweep different classifier-free guidance strength with selected intervals. Our DDT-XL/2 achieves the best performance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] with a classifer-free guidance of 2. Recall that we donate t=0𝑡0t=0italic_t = 0 as the pure noise timestep while REPA[52] use t=1𝑡1t=1italic_t = 1, thus this exactly correspond to the [0,0.7]00.7[0,0.7][ 0 , 0.7 ] interval in REPA[52] Figure 9: FID10K of DDT-XL/2 with different Classifer free guidance strength and guidance intervals. We sweep different classifier-free guidance strength with selected intervals. Our DDT-XL/2 achieves the best performance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] with a classifer-free guidance of 2."
  },
  {
    "title": "Instructions for reporting errors",
    "level": 2,
    "content": "We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below: Click the \"Report Issue\" button. Open a report feedback form via keyboard, use \"Ctrl + ?\". Make a text selection and click the \"Report Issue for Selection\" button near your cursor. You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section. Our team has already identified the following issues. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all. Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions."
  }
]