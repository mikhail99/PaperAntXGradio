[
  {
    "title": "Table of Contents",
    "level": 2,
    "content": "Abstract 1 Introduction 2 Related Work Diffusion Transformers. Fast Diffusion Training. 3 Preliminary Analysis 4 Method 4.1 Condition Encoder 4.2 Velocity Decoder 4.3 Sampling acceleration Uniform Encoder Sharing. Statistic Dynamic Programming. 5 Experiment 5.1 Improved baselines 5.2 Metric comparison with baselines 5.3 System level comparision ImageNet 256Ã—256256256256\\times 256256 Ã— 256. ImageNet 512Ã—512512512512\\times 512512 Ã— 512 5.4 Acceleration by Encoder sharing 5.5 Ablations Encoder-Decoder Ratio Decoder Block types. 6 Conclusion A Model Specs B Hyper-parameters C Linear flow and Diffusion D Proof of Spectrum Autoregressive E Linear multisteps method F Classifier free guidance. References"
  },
  {
    "title": "DDT: Decoupled Diffusion Transformer",
    "level": 1,
    "content": "Shuai Wang1 Zhi Tian2 Weilin Huang2 Limin Wang 1, \\faEnvelope 1Nanjing University 2ByteDance Seed Vision https://github.com/MCG-NJU/DDT Figure 1: Our deoupled diffusion transformer (DDT-XL/2) achieves a SoTA 1.31 FID under 256 epochs. Our decoupled diffusion transformer models incorporate a condition encoder to extract semantic self-conditions and a velocity decoder to decode velocity. â€ â€  \\faEnvelope : Corresponding author (lmwang@nju.edu.cn)."
  },
  {
    "title": "Abstract",
    "level": 6,
    "content": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new Decoupled Diffusion Transformer (DDT), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256Ã—256256256256\\times 256256 Ã— 256, Our DDT-XL/2 achieves a new state-of-the-art performance of 1.31 FID (nearly 4Ã—4\\times4 Ã— faster training convergence compared to previous diffusion transformers). For ImageNet 512Ã—512512512512\\times 512512 Ã— 512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies."
  },
  {
    "title": "1 Introduction",
    "level": 2,
    "content": "Image generation is a fundamental task in computer vision research, which aims at capturing the inherent data distribution of original image datasets and generating high-quality synthetic images through distribution sampling. Diffusion models [19, 41, 21, 30, 29] have recently emerged as highly promising solutions to learn the underlying data distribution in image generation, outperforming the GAN-based models [3, 40] and Auto-Regressive models [5, 43, 51]. The diffusion forward process gradually adds Gaussian noise to the pristine data following an SDE forward schedule [19, 41, 21]. The denoising process learns the score estimation from this corruption process. Once the score function is accurately learned, data samples can be synthesized by numerically solving the reverse SDE [41, 21, 30, 29]. Diffusion Transformers [36, 32] introduce the transformer architecture into diffusion models to replace the traditionally dominant UNet-based model [2, 10]. Empirical evidence suggests that, given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections [36]. Nevertheless, their slow convergence rate still poses great challenge for developing new models due to the high cost. In this paper, we want to tackle the aforementioned major disadvantages from a model design perspective. Classic computer vision algorithms [4, 23, 17] strategically employ encoder-decoder architectures, prioritizing large encoders for rich feature extraction and lightweight decoders for efficient inference, while contemporary diffusion models predominantly rely on conventional decoder-only structures. We systematically investigate the underexplored potential of decoupled encoder-decoder designs in diffusion transformers, by answering the question of can decoupled encoder-decoder transformer unlock the capability of accelerated convergence and enhanced sample quality? Through investigation experiments, we conclude that the plain diffusion transformer has an optimization dilemma between abstract structure information extraction and detailed appearance information recovery. Further, the diffusion transformer is limited in extracting semantic representation due to the raw pixel supervision [52, 53, 28]. To address this issue, we propose a new architecture to explicitly decouple low-frequency semantic encoding and high-frequency detailed decoding through a customized encoder-decoder design. We call this encoder-decoder diffusion transformer model as DDT (Decoupled Diffusion Transformer). DDT incorporates a condition encoder to extract semantic self-condition features. The extracted self-condition is fed into a velocity decoder along with the noisy latent to regress the velocity field. To maintain the local consistency of self-condition features of adjacent steps, we employ direct supervision of representation alignment and indirect supervision from the velocity regression loss of the decoder. In the ImageNet256Ã—256256256256\\times 256256 Ã— 256 dataset, using the traditional off-shelf VAE [38], our decoupled diffusion transformer (DDT-XL/2) model achieves the state-of-the-art performance of 1.31 FID with interval guidance under only 256 epochs, approximately 4Ã—4\\times4 Ã— training acceleration compared to REPA [52]. In the ImageNet512Ã—512512512512\\times 512512 Ã— 512 dataset, our DDT-XL/2 model achieves 1.28 FID within 500K finetuning steps. Furthermore, our DDT achieves strong local consistency on its self-condition feature from the encoder. This property can significantly boost the inference speed by sharing the self-condition between adjacent steps. We formulate the optimal encoder sharing strategy solving as a classic minimal sum path problem by minimizing the performance drop of sharing self-condition among adjacent steps. We propose a statistic dynamic programming approach to find the optimal encoder sharing strategy with negligible second-level time cost. Compared with the naive uniform sharing, our dynamic programming delivers a minimal FID drop. Our contributions are summarized as follows. â€¢ We propose a new decoupled diffusion transformer model, which consists of a condition encoder and a velocity decoder. â€¢ We propose statistic dynamic programming to find the optimal self-condition sharing strategy to boost inference speed while keeping minimal performance down-gradation. â€¢ In the ImageNet256Ã—256256256256\\times 256256 Ã— 256 dataset, using tradition SDf8d4 VAE, our decoupled diffusion transformer (DDT-XL/2) model achieves the SoTA 1.31 FID with interval guidance under only 256 epochs, approximately 4Ã—4\\times4 Ã— training acceleration compared to REPA [52]. â€¢ In the ImageNet512Ã—512512512512\\times 512512 Ã— 512 dataset, our DDT-XL/2 model achieves the SoTA 1.28 FID, outperforming all previous methods with a significant margin."
  },
  {
    "title": "2 Related Work",
    "level": 2,
    "content": "Figure 2: Selected 256Ã—256256256256\\times 256256 Ã— 256 and 512Ã—512512512512\\times 512512 Ã— 512 resolution samples. Generated from DDT-XL/2 trained on ImageNet 256Ã—256256256256\\times 256256 Ã— 256 resolution and ImageNet 512Ã—512512512512\\times 512512 Ã— 512 resolution with CFG = 4.0."
  },
  {
    "title": "Diffusion Transformers.",
    "level": 4,
    "content": "The pioneering work of DiT [36] introduced transformers into diffusion models to replace the traditionally dominant UNet architecture [2, 10]. Empirical evidence demonstrates that given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections. SiT [32] further validated the transformer architecture with linear flow diffusion. Following the simplicity and scalability of the diffusion transformer [32, 36], SD3 [12], Lumina [54], and PixArt [6, 7] introduced the diffusion transformer to more advanced text-to-image areas. Moreover, recently, diffusion transformers have dominated the text-to-video area with substantiated visual and motion quality [24, 1, 20]. Our decoupled diffusion transformer (DDT) presents a new variant within the diffusion transformer family. It achieves faster convergence by decoupling the low-frequency encoding and the high-frequency decoding."
  },
  {
    "title": "Fast Diffusion Training.",
    "level": 4,
    "content": "To accelerate the training efficiency of diffusion transformers, recent advances have pursued multi-faceted optimizations. Operator-centric approaches [13, 48, 49, 45] leverage efficient attention mechanisms: linear-attention variants [13, 49, 45] reduced quadratic complexity to speed up training, while sparse-attention architectures [48] prioritized sparsely relevant token interactions. Resampling approaches [12, 16] proposed lognorm sampling [12] or loss reweighting [16] techniques to stabilize training dynamics. Representation learning enhancement approaches integrate external inductive biases: REPA [52], RCG [27] and DoD [53] borrowed vision-specific priors into diffusion training, while masked modeling techniques [14, 15] strengthened spatial reasoning by enforcing structured feature completion during denoising. Collectively, these strategies address computational, sampling, and representational bottlenecks."
  },
  {
    "title": "3 Preliminary Analysis",
    "level": 2,
    "content": "Figure 3: The reverse-SDE process (generation) of SiT-XL/2 in xğ‘¥xitalic_x space. There is a clear generation process from low frequency to high frequency. Most of the time is spent on generating high-frequency details (from t=0.4ğ‘¡0.4t=0.4italic_t = 0.4 to t=1.0ğ‘¡1.0t=1.0italic_t = 1.0). Figure 4: The FID50K metric of SiT-XL/2 for different timeshift values. We employ a 2222-nd order Adams-like solver to collect the performance. Allocating more computation at noisy steps significantly improves the performance. Linear-based flow matching [30, 29, 32] represents a specialized family of diffusion models that we focus on as our primary analytical subject due to its simplicity and efficiency. For the convenience of discussion, in certain situations, diffusion and flow-matching will be used interchangeably. In this framework, t=0ğ‘¡0t=0italic_t = 0 corresponds to the pure noise timestep. As illustrated in Fig. 3, diffusion models perform autoregressive refinement on spectral components [37, 11]. The diffusion transformer encodes the noisy latent to capture lower-frequency semantics before decoding higher-frequency details. However, this semantics encoding process inevitably attenuates high-frequency information, creating an optimization dilemma. This observation motivates our proposal to decouple the conventional decode-only diffusion transformer into an explicit encoder-decoder architecture. Eq. 1 is directly borrowed from [37, 11], we place the proof of Eq. 1 in Appendix. According to Eq. 1, as tğ‘¡titalic_t increases to less noisy timesteps, semantic encoding becomes easier (due to noise reduction) while decoding complexity increases (as residual frequencies grow). Consider the worst-case scenario at denoising step tğ‘¡titalic_t, the diffusion transformer encodes frequencies up to fmâ¢aâ¢xâ¢(t)subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ), to progress to step sğ‘ sitalic_s, it must decode a residual frequency of at least fmâ¢aâ¢xâ¢(s)âˆ’fmâ¢aâ¢xâ¢(t)subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘ subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡f_{max}(s)-f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_s ) - italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ). Failure to decode these residual frequencies at step tğ‘¡titalic_t creates a critical bottleneck for progression to subsequent steps. From this perspective, if allocating more of the calculations to more noisy timesteps can lead to an improvement, it means that diffusion transformers struggle with encoding lower frequency to provide semantics. Otherwise, if allocating more of the calculations to less noisy timesteps can lead to an improvement, it means that flow-matching transformers struggle with decoding higher frequency to provide fine details. To figure out the bottom-necks of current diffusion models, we conducted a targeted experiment using SiT-XL/2 with a second-order Adams-like linear multistep solver. As shown in Fig. 4, by varying the time-shift values, we demonstrate that allocating more computation to early timesteps improves final performance compared to uniform scheduling. This reveals that diffusion models face challenges in more noisy steps. This leads to a key conclusion: Current diffusion transformers are fundamentally constrained by their low-frequency semantic encoding capacity. This insight motivates the exploration of encoder-decoder architectures with strategic encoder parameter allocation. Prior researches further support this perspective. While lightweight diffusion MLP heads demonstrate limited decoding capacity, MAR [28] overcomes this limitation through semantic latents produced by its masked backbones, enabling high-quality image generation. Similarly, REPA [52] enhances low-frequency encoding through alignment with pre-trained vision foundations [35]."
  },
  {
    "title": "Lemma 1.",
    "level": 6,
    "content": "For a linear flow-matching noise scheduler at timestep tğ‘¡titalic_t, let us denote Kfâ¢râ¢eâ¢qsubscriptğ¾ğ‘“ğ‘Ÿğ‘’ğ‘K_{freq}italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT as the maximum frequency of the clean data ğ±dâ¢aâ¢tâ¢asubscriptğ±ğ‘‘ğ‘ğ‘¡ğ‘{\\boldsymbol{x}}_{data}bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT. The maximum retained frequency in the noisy latent satisfies: fmâ¢aâ¢xâ¢(t)>minâ¡((t1âˆ’t)2,Kfâ¢râ¢eâ¢q).subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡superscriptğ‘¡1ğ‘¡2subscriptğ¾ğ‘“ğ‘Ÿğ‘’ğ‘f_{max}(t)>\\min\\left({\\left(\\frac{t}{1-t}\\right)}^{2},K_{freq}\\right).italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_t end_ARG start_ARG 1 - italic_t end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) . (1)"
  },
  {
    "title": "4 Method",
    "level": 2,
    "content": "Our decoupled diffusion transformer architecture comprises a condition encoder and a velocity decoder. The condition encoder extracted the low-frequency component from noisy input, class label, and timestep to serve as a self-condition for the velocity decoder; the velocity decoder processed the noisy latent with the self-condition to regress the high-frequency velocity. We train this model using the established linear flow diffusion framework. For brevity, we designate our model as DDT (Decoupled Diffusion Transformer)."
  },
  {
    "title": "4.1 Condition Encoder",
    "level": 3,
    "content": "The condition encoder mirrors the architectural design and input structure of DiT/SiT with improved micro-design. It is built with interleaved Attention and FFN blocks, without long residual connections. The encoder processes three inputs, the noisy latent ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, timestep tğ‘¡titalic_t, and class label yğ‘¦yitalic_y, to extract the self-condition feature ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT through a series of stacked Attention and FFN blocks: ğ’›t=Encoderâ¢(ğ’™t,t,y).subscriptğ’›ğ‘¡Encodersubscriptğ’™ğ‘¡ğ‘¡ğ‘¦{\\boldsymbol{z}}_{t}=\\textbf{Encoder}~{}({\\boldsymbol{x}}_{t},t,y).bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Encoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_y ) . (2) Specifically, the noisy latent ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are patchfied into continuous tokens and then fed to extract the self-condition ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with aforementioned encoder blocks. The timestep tğ‘¡titalic_t and class label yğ‘¦yitalic_y serve as external-conditioning information projected into embedding. These external-condition embeddings are progressively injected into the encoded features of ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT using AdaLN-Zero[36] within each encoder block. To maintain local consistency of ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT across adjacent timesteps, we adopt the representation alignment technique from REPA [52]. Shown in Eq. 3, this method aligns the intermediate feature ğ¡isubscriptğ¡ğ‘–\\mathbf{h}_{i}bold_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT from the iğ‘–iitalic_i-th layer in the self-mapping encoder with the DINOv2 representation râˆ—subscriptğ‘Ÿr_{*}italic_r start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT. Consistent to REPA [52], the hÏ•subscriptâ„italic-Ï•h_{\\phi}italic_h start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT is the learnable projection MLP: â„’eâ¢nâ¢c=1âˆ’cosâ¡(râˆ—,hÏ•â¢(ğ¡ğ¢)).subscriptâ„’ğ‘’ğ‘›ğ‘1subscriptğ‘Ÿsubscriptâ„italic-Ï•subscriptğ¡ğ¢\\mathcal{L}_{enc}=1-\\cos(r_{*},h_{\\phi}(\\mathbf{h_{i}})).caligraphic_L start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT = 1 - roman_cos ( italic_r start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ( bold_h start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT ) ) . (3) This simple regularization accelerates training convergence, as shown in REPA [52], and facilitates local consistency of ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT between adjacent steps. It allows sharing the self-condition ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT produced by the encoder between adjacent steps. Our experiments demonstrate that this encoder-sharing strategy significantly enhances inference efficiency with only negligible performance degradation. Additionally, the encoder also receives indirect supervision from the decoder, which we elaborate on later."
  },
  {
    "title": "4.2 Velocity Decoder",
    "level": 3,
    "content": "The velocity decoder adopts the same architectural design as the condition encoder and consists of several stacked interleaved Attention and FFN blocks, akin to DiT/SiT. It takes the noisy latent ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, timestep tğ‘¡titalic_t, and self-conditioning ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as inputs to estimate the velocity ğ’—tsubscriptğ’—ğ‘¡{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Unlike the encoder, we assume that class label information is already embedded within ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Thus, only the external-condition timestep tğ‘¡titalic_t and self-condition feature ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are used as condition inputs for the decoder blocks: ğ’—t=Decoderâ¢(ğ’™t,t,ğ’›t).subscriptğ’—ğ‘¡Decodersubscriptğ’™ğ‘¡ğ‘¡subscriptğ’›ğ‘¡{\\boldsymbol{v}}_{t}=\\textbf{Decoder}~{}({\\boldsymbol{x}}_{t},t,{\\boldsymbol{z% }}_{t}).bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Decoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (4) As demonstrated previously, to further improve consistency of self-condition ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT between adjacent steps, we employ AdaLN-Zero [36] to inject ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT into the decoder feature. The decoder is trained with the flow matching loss as shown in Eq. 5: â„’dâ¢eâ¢c=ğ”¼â¢[âˆ«01â€–(ğ’™dâ¢aâ¢tâ¢aâˆ’Ïµ)âˆ’ğ’—tâ€–2â¢dt].subscriptâ„’ğ‘‘ğ‘’ğ‘ğ”¼delimited-[]superscriptsubscript01superscriptnormsubscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘italic-Ïµsubscriptğ’—ğ‘¡2differential-dğ‘¡\\mathcal{L}_{dec}=\\mathbb{E}[\\int_{0}^{1}||({\\boldsymbol{x}}_{data}-{\\epsilon}% )-{\\boldsymbol{v}}_{t}||^{2}\\mathrm{d}t].caligraphic_L start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = blackboard_E [ âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT | | ( bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT - italic_Ïµ ) - bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_d italic_t ] . (5)"
  },
  {
    "title": "4.3 Sampling acceleration",
    "level": 3,
    "content": "By incorporating explicit representation alignment into the encoder and implicit self-conditioning injection into the decoder, we achieve local consistency of ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT across adjacent steps during training (shown in Fig. 5). This enables us to share ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT within a suitable local range, reducing the computational burden on the self-mapping encoder. Formally, given total inference steps Nğ‘Nitalic_N and encoder computation bugets Kğ¾Kitalic_K, thus the sharing ratio is 1âˆ’KN1ğ¾ğ‘1-\\frac{K}{N}1 - divide start_ARG italic_K end_ARG start_ARG italic_N end_ARG, we define Î¦Î¦\\Phiroman_Î¦ with |Î¦|=KÎ¦ğ¾|\\Phi|=K| roman_Î¦ | = italic_K as the set of timesteps where the self-condition is recalculated, as shown in Equation 6. If the current timestep tğ‘¡titalic_t is not in Î¦Î¦\\Phiroman_Î¦, we reuse the previously computed ğ’›tâˆ’Î”â¢tsubscriptğ’›ğ‘¡Î”ğ‘¡{\\boldsymbol{z}}_{t-\\Delta t}bold_italic_z start_POSTSUBSCRIPT italic_t - roman_Î” italic_t end_POSTSUBSCRIPT as ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Otherwise, we recompute ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT using the encoder and the current noisy latent ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT: ğ’›t={ğ’›tâˆ’Î”â¢t,if â¢tâˆ‰Î¦Encoderâ¢(ğ’™t,t,y),if â¢tâˆˆÎ¦subscriptğ’›ğ‘¡casessubscriptğ’›ğ‘¡Î”ğ‘¡if ğ‘¡Î¦Encodersubscriptğ’™ğ‘¡ğ‘¡ğ‘¦if ğ‘¡Î¦{\\boldsymbol{z}}_{t}=\\begin{cases}{\\boldsymbol{z}}_{t-\\Delta t},&\\text{if }t% \\notin\\Phi\\\\ \\textbf{Encoder}~{}({\\boldsymbol{x}}_{t},t,y),&\\text{if }t\\in\\Phi\\end{cases}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT italic_t - roman_Î” italic_t end_POSTSUBSCRIPT , end_CELL start_CELL if italic_t âˆ‰ roman_Î¦ end_CELL end_ROW start_ROW start_CELL Encoder ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_y ) , end_CELL start_CELL if italic_t âˆˆ roman_Î¦ end_CELL end_ROW (6)"
  },
  {
    "title": "Uniform Encoder Sharing.",
    "level": 4,
    "content": "This naive approach recaluculate self-condition ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT every NKğ‘ğ¾\\frac{N}{K}divide start_ARG italic_N end_ARG start_ARG italic_K end_ARG steps. Previous work, such as DeepCache [33], uses this naive handcrafted uniform Î¦Î¦\\Phiroman_Î¦ set to accelerate UNet models. However, UNet models, trained solely with a denoising loss and lacking robust representation alignment, exhibit less regularized local consistency in deeper features across adjacent steps compared to our DDT model. Also, we will propose a simple and elegant statistic dynamic programming algorithm to construct Î¦Î¦\\Phiroman_Î¦. Our statistic dynamic programming can exploit the optimal Î¦Î¦\\Phiroman_Î¦ set optimally compared to the naive approaches [33]."
  },
  {
    "title": "Statistic Dynamic Programming.",
    "level": 4,
    "content": "We construct the statistic similarity matrix of ztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT among different steps ğ’âˆˆRNÃ—Nğ’superscriptğ‘…ğ‘ğ‘\\mathbf{S}\\in R^{N\\times N}bold_S âˆˆ italic_R start_POSTSUPERSCRIPT italic_N Ã— italic_N end_POSTSUPERSCRIPT using cosine distance. The optimal Î¦Î¦\\Phiroman_Î¦ set would guarantee the total similarity cost âˆ’âˆ‘kKâˆ‘i=Î¦kÎ¦k+1Sâ¢[Î¦k,i]superscriptsubscriptğ‘˜ğ¾superscriptsubscriptğ‘–subscriptÎ¦ğ‘˜subscriptÎ¦ğ‘˜1ğ‘†subscriptÎ¦ğ‘˜ğ‘–-\\sum_{k}^{K}\\sum_{i=\\Phi_{k}}^{\\Phi_{k+1}}S[\\Phi_{k},i]- âˆ‘ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i = roman_Î¦ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_Î¦ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_S [ roman_Î¦ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_i ] achieves global minimal. This question is a well-formed classic minimal sum path problem, it can be solved by dynamic programming. As shown in Eq. 8, we donate ğ‚iksubscriptsuperscriptğ‚ğ‘˜ğ‘–\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as cost and ğiksubscriptsuperscriptğğ‘˜ğ‘–\\mathbf{P}^{k}_{i}bold_P start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as traced path when Î¦k=isubscriptÎ¦ğ‘˜ğ‘–\\Phi_{k}=iroman_Î¦ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_i. the state transition function from ğ‚jkâˆ’1subscriptsuperscriptğ‚ğ‘˜1ğ‘—\\mathbf{C}^{k-1}_{j}bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT to ğ‚iksubscriptsuperscriptğ‚ğ‘˜ğ‘–\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT follows: ğ‚iksubscriptsuperscriptğ‚ğ‘˜ğ‘–\\displaystyle\\mathbf{C}^{k}_{i}bold_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =minj=0iâ¡{ğ‚jkâˆ’1âˆ’Î£l=jiâ¢ğ’â¢[j,l]}.absentsuperscriptsubscriptğ‘—0ğ‘–subscriptsuperscriptğ‚ğ‘˜1ğ‘—superscriptsubscriptÎ£ğ‘™ğ‘—ğ‘–ğ’ğ‘—ğ‘™\\displaystyle=\\min_{j=0}^{i}\\{\\mathbf{C}^{k-1}_{j}-\\Sigma_{l=j}^{i}\\mathbf{S}[% j,l]\\}.= roman_min start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT { bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - roman_Î£ start_POSTSUBSCRIPT italic_l = italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_S [ italic_j , italic_l ] } . (7) ğiksubscriptsuperscriptğğ‘˜ğ‘–\\displaystyle\\mathbf{P}^{k}_{i}bold_P start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =argminj=0iâ¡{ğ‚ikâˆ’1âˆ’Î£l=jiâ¢ğ’â¢[j,l]}.absentsuperscriptsubscriptargminğ‘—0ğ‘–subscriptsuperscriptğ‚ğ‘˜1ğ‘–superscriptsubscriptÎ£ğ‘™ğ‘—ğ‘–ğ’ğ‘—ğ‘™\\displaystyle=\\operatorname{argmin}_{j=0}^{i}\\{\\mathbf{C}^{k-1}_{i}-\\Sigma_{l=% j}^{i}\\mathbf{S}[j,l]\\}.= roman_argmin start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT { bold_C start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_Î£ start_POSTSUBSCRIPT italic_l = italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_S [ italic_j , italic_l ] } . (8) After obtaining the cost matrix ğ‚ğ‚\\mathbf{C}bold_C and tracked path ğğ\\mathbf{P}bold_P, the optimal Î¦Î¦\\Phiroman_Î¦ can be solved by backtracking ğğ\\mathbf{P}bold_P from ğNKsuperscriptsubscriptğğ‘ğ¾\\mathbf{P}_{N}^{K}bold_P start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT."
  },
  {
    "title": "5 Experiment",
    "level": 2,
    "content": "We conduct experiments on 256x256 ImageNet datasets. The total training batch size is set to 256. Consistent with methodological approaches such as SiT [32], DiT [36], and REPA [52], we employed the Adam optimizer with a constant learning rate of 0.0001 throughout the entire training process. To ensure a fair comparative analysis, we did not use gradient clipping and learning rate warm-up techniques. Our default training infrastructure consisted of 16Ã—16\\times16 Ã— or 8Ã—8\\times8 Ã— A100 GPUs. For sampling, we take the Euler solver with 250 steps as the default choice. As for the VAE, we take the off-shelf VAE-ft-EMA with a downsample factor of 8 from Huggingface111https://huggingface.co/stabilityai/sd-vae-ft-ema. We report FID [18], sFID [34], IS [39], Precision and Recall [25]. 256Ã—\\timesÃ—256, w/o CFG 256Ã—\\timesÃ—256, w/ CFG Params Epochs FIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Pre.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ FIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Pre.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ MAR-B [28] 208M 800 3.48 192.4 0.78 0.58 2.31 281.7 0.82 0.57 CausalFusion [9] 368M 800 5.12 166.1 0.73 0.66 1.94 264.4 0.82 0.59 LDM-4 [38] 400M 170 10.56 103.5 0.71 0.62 3.6 247.7 0.87 0.48 DDT-L (Ours) 458M 80 7.98 128.1 0.68 0.67 1.64 310.5 0.81 0.61 MAR-L [28] 479M 800 2.6 221.4 0.79 0.60 1.78 296.0 0.81 0.60 VAVAE [50] 675M 800 2.17 205.6 0.77 0.65 1.35 295.3 0.79 0.65 CausalFusion [9] 676M 800 3.61 180.9 0.75 0.66 1.77 282.3 0.82 0.61 ADM [10] 554M 400 10.94 - 0.69 0.63 4.59 186.7 0.82 0.52 DiT-XL [36] 675M 1400 9.62 121.5 0.67 0.67 2.27 278.2 0.83 0.57 SiT-XL [32] 675M 1400 8.3 - - - 2.06 270.3 0.82 0.59 ViT-XL [16] 451M 400 8.10 - - - 2.06 - - - U-ViT-H/2 [2] 501M 400 6.58 - - - 2.29 263.9 0.82 0.57 MaskDiT [14] 675M 1600 5.69 178.0 0.74 0.60 2.28 276.6 0.80 0.61 FlowDCN [48] 618M 400 8.36 122.5 0.69 0.65 2.00 263.1 0.82 0.58 RDM [44] 553M / 5.27 153.4 0.75 0.62 1.99 260.4 0.81 0.58 REPA [52] 675M 800 5.9 157.8 0.70 0.69 1.42 305.7 0.80 0.64 DDT-XL (Ours) 675M 80 6.62 135.2 0.69 0.67 1.52 263.7 0.78 0.63 DDT-XL (Ours) 675M 256 6.30 146.7 0.68 0.68 1.31 308.1 0.78 0.62 DDT-XL (Ours) 675M 400 6.27 154.7 0.68 0.69 1.26 310.6 0.79 0.65 Table 1: System performance comparison on ImageNet 256Ã—256256256256\\times 256256 Ã— 256 class-conditioned generation. Gray blocks mean the algorithm uses VAE trained or fine-tuned on ImageNet instead of the off-shelf SD-VAE-f8d4-ft-ema."
  },
  {
    "title": "5.1 Improved baselines",
    "level": 3,
    "content": "Recent architectural improvements such as SwiGLU [46, 47], RoPE [42], and RMSNorm [46, 47] have been extensively validated in the research community [8, 50, 31]. Additionally, lognorm sampling [12] has demonstrated significant benefits for training convergence. Consequently, we developed improved baseline models by incorporating these advanced techniques, drawing inspiration from recent works in the field. The performance of these improved baselines is comprehensively provided in Tab. 2. To validate the reliability of our implementation, we also reproduced the results for REPA-B/2, achieving metrics that marginally exceed those originally reported in the REPA[52]. These reproduction results provide additional confidence in the robustness of our approach. The improved baselines in our Tab. 2 consistently outperform their predecessors without REPA. However, upon implementing REPA, performance rapidly approaches a saturation point. This is particularly evident in the XL model size, where incremental technique improvements yield diminishingly small gains."
  },
  {
    "title": "5.2 Metric comparison with baselines",
    "level": 3,
    "content": "We present the performances of different-size models at 400K training steps in Tab. 2. Our diffusion encoder-decoder transformer(DDT) family demonstrates consistent and significant improvements across various model sizes. Our DDT-B/2(8En4De) model exceeds Improved-REPA-B/2 by 2.8 FID gains. Our DDT-XL/2(22En6De) exceeds REPA-XL/2 by 1.3 FID gains. While the decoder-only diffusion transformers approach performance saturation with REPA[52], our DDT models continue to deliver superior results. The incremental technique improvements show diminishing gains, particularly in larger model sizes. However, our DDT models maintain a significant performance advantage, underscoring the effectiveness of our approach. Model FIDâ†“â†“\\downarrowâ†“ sFIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Prec.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ SiT-B/2 [32] 33.0 6.46 43.7 0.53 0.63 REPA-B/2 [52] 24.4 6.40 59.9 0.59 0.65 REPA-B/2(Reproduced) 22.2 7.50 69.1 0.59 0.65 DDT-B/2â€  (8En4De) 21.1 7.81 73.0 0.60 0.65 Improved-SiT-B/2 25.1 6.54 58.8 0.57 0.64 Improved-REPA-B/2 19.1 6.88 76.49 0.60 0.66 DDT-B/2 (8En4De) 16.32 6.63 86.0 0.62 0.66 SiT-L/2 [32] 18.8 5.29 72.0 0.64 0.64 REPA-L/2 [52] 10.0 5.20 109.2 0.69 0.65 Improved-SiT-L/2 12.7 5.48 95.7 0.65 0.65 Improved-REPA-L/2 9.3 5.44 116.6 0.67 0.66 DDT-L/2 (20En4De) 7.98 5.50 128.1 0.68 0.67 SiT-XL/2 [32] 17.2 5.07 76.52 0.65 0.63 REPA-XL/2 [52] 7.9 5.06 122.6 0.70 0.65 Improved-SiT-XL/2 10.9 5.3 103.4 0.66 0.65 Improved-REPA-XL/2 8.14 5.34 124.9 0.68 0.67 DDT-XL/2 (22En6De) 6.62 4.86 135.1 0.69 0.67 Table 2: Metrics of 400â¢K400ğ¾400K400 italic_K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. By default, our DDT models are built on improved baselines. DDTâ€  means model built on naive baseline without architecture improvement and lognorm sampling, consistent to REPA. Our DDT models consistently outperformed their counterparts."
  },
  {
    "title": "ImageNet 256Ã—256256256256\\times 256256 Ã— 256.",
    "level": 4,
    "content": "We report the final metrics of DDT-XL/2 (22En6De) and DDT-L/2 (20En4De) at Tab. 1. Our DDT models demonstrate exceptional efficiency, achieving convergence in approximately 1414\\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG of the total epochs compared to REPA [52] and other diffusion transformer models. In order to maintain methodological consistency with REPA, we employed the classifier-free guidance with 2.0 in the interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ], Our models delivered impressive results: DDT-L/2 achieved 1.64 FID, and DDT-XL/2 got 1.52 FID within just 80 epochs. By extending training to 256 epochsâ€”still significantly more efficient than traditional 800-epoch approachesâ€”our DDT-XL/2 established a new state-of-the-art benchmark of 1.31 FID on ImageNet 256Ã—256, decisively outperforming previous diffusion transformer methodologies. To extend training to 400400400400 epochs, our DDT-XL/2(22En6De) achieves 1.26 FID, nearly reaching the upper limit of SD-VAE-ft-EMA-f8d4, which has a 1.20 rFID on ImageNet256256256256."
  },
  {
    "title": "ImageNet 512Ã—512512512512\\times 512512 Ã— 512",
    "level": 4,
    "content": "We provide the final metrics of DDT-XL/2 at Tab. 3. To validate the superiority of our DDT model, we take our DDT-XL/2 trained on ImageNet 256Ã—256256256256\\times 256256 Ã— 256 under 256 epochs as the initialization, fine-tune out DDT-XL/2 on ImageNet 512Ã—512512512512\\times 512512 Ã— 512 for 100â¢K100ğ¾100K100 italic_K steps. We adopt the aforementioned interval guidance [26] and we achieved a remarkable state-of-the-art performance of 1.90 FID, decisively outperforming REPA by a significant 0.28 performance margin. In Tab. 3, some metrics exhibit subtle degradation, we attribute this to potentially insufficient fine-tuning. When allocating more training iterations to DDT-XL/2, it achieves 1.281.281.281.28 FID at 500K steps with CFG3.0 within the time interval [0.3,1.0]0.31.0[0.3,1.0][ 0.3 , 1.0 ]. ImageNet 512Ã—512512512512\\times 512512 Ã— 512 Model FIDâ†“â†“\\downarrowâ†“ sFIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Pre.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ BigGAN-deep [3] 8.43 8.13 177.90 0.88 0.29 StyleGAN-XL [40] 2.41 4.06 267.75 0.77 0.52 ADM-G [10] 7.72 6.57 172.71 0.87 0.42 ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53 DiT-XL/2 [36] 3.04 5.02 240.82 0.84 0.54 SiT-XL/2 [32] 2.62 4.18 252.21 0.84 0.57 REPA-XL/2 [52] 2.08 4.19 274.6 0.83 0.58 FlowDCN-XL/2 [48] 2.44 4.53 252.8 0.84 0.54 DDT-XL/2 (500K) 1.28 4.22 305.1 0.80 0.63 Table 3: Benchmarking class-conditional image generation on ImageNet 512Ã—\\timesÃ—512. Our DDT-XL/2(512Ã—512512512512\\times 512512 Ã— 512) is fine-tuned from the same model trained on 256Ã—256256256256\\times 256256 Ã— 256 resolution setting of 1.28M steps. We adopt the interval guidance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] and CFG of 3.0"
  },
  {
    "title": "5.4 Acceleration by Encoder sharing",
    "level": 3,
    "content": "As illustrated in Fig. 5, there is a strong local consistency of the self-condition in our condition encoder. Even ğ’›t=0subscriptğ’›ğ‘¡0{\\boldsymbol{z}}_{t=0}bold_italic_z start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT has a strong similarity above 0.8 with ğ’›t=1subscriptğ’›ğ‘¡1{\\boldsymbol{z}}_{t=1}bold_italic_z start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT. This consistency provides an opportunity to speed up inference by sharing the encoder between adjacent steps. We employed the simple uniform encoder sharing strategy and the new novel statistics dynamic programming strategy. Specifically, for the uniform strategy, we only recalculate the self-condition ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT every Kğ¾Kitalic_K steps. For statistics dynamic programming, we solve the aforementioned minimal sum path on the similarity matrix by dynamic programming and recalculate ğ’›tsubscriptğ’›ğ‘¡{\\boldsymbol{z}}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT according to the solved strategy. As shown in Fig. 6, there is a significant inference speedup nearly without visual quality loss when Kğ¾Kitalic_K is smaller than 6. As shown in Tab. 4, the metrics loss is still marginal, while the inference speedup is significant. The novel statistics dynamic programming slightly outperformed the naive uniform strategy with less FID drop. SharRatio Acc Î¦Î¦\\Phiroman_Î¦ FIDâ†“â†“\\downarrowâ†“ sFIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Prec.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ 0.00 1.0Ã—1.0\\times1.0 Ã— Uniform 1.31 4.62 308.1 0.78 0.66 0.50 1.6Ã—1.6\\times1.6 Ã— Uniform 1.31 4.48 300.5 0.78 0.65 0.66 1.9Ã—1.9\\times1.9 Ã— Uniform 1.32 4.46 301.2 0.78 0.65 0.75 2.3Ã—2.3\\times2.3 Ã— Uniform 1.34 4.43 302.7 0.78 0.65 0.80 2.6Ã—2.6\\times2.6 Ã— Uniform 1.36 4.40 303.3 0.78 0.64 StatisticDP 1.33 4.37 301.7 0.78 0.64 0.83 2.7Ã—2.7\\times2.7 Ã— Uniform 1.37 4.41 302.8 0.78 0.64 StatisticDP 1.36 4.35 300.3 0.78 0.64 0.87 3.0Ã—3.0\\times3.0 Ã— Uniform 1.42 4.43 302.8 0.78 0.64 StatisticDP 1.40 4.35 302.4 0.78 0.64 Table 4: Metrics of 400â¢K400ğ¾400K400 italic_K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. Our DDT models consistently outperformed its counterparts Figure 5: The cosine similarity of self-condition feature ztsubscriptğ‘§ğ‘¡\\boldsymbol{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from encoder between different timesteps. There is a strong correlation between adjacent steps, indicating the redundancy. Figure 6: Sharing the self-condition ztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in adjacent steps significant speedup the inference.We tried various sharing frequency configurations. There is marginal visual quality down-gradation when the sharing frequency is reasonable."
  },
  {
    "title": "5.5 Ablations",
    "level": 3,
    "content": "(a) (b) (c) Figure 7: The DDT-B/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-B/2(8En4De) achieves much faster convergence speed and better performance. (a) (b) (c) Figure 8: The DDT-L/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-L/2 prefers an unexpected aggressive encoder-deocder ratio DDT-L/2(20En4De) achieves much faster convergence speed and better performance. We conduct ablation studies on ImageNet 256Ã—256256256256\\times 256256 Ã— 256 with DDT-B/2 and DDT-L/2. For sampling, we take the Euler solver with 250 steps as the default choice without classifier-free guidance. For training, we train each model with 80 epochs(400k steps), and the batch size is set to 256."
  },
  {
    "title": "Encoder-Decoder Ratio",
    "level": 4,
    "content": "we systematically explored ratios ranging from 2:1:212:12 : 1 to 5:1:515:15 : 1 across different model sizes. in Fig. 7 and Fig. 8. Our notation mğ‘šmitalic_mEnnğ‘›nitalic_nDe represents models with mğ‘šmitalic_m encoder layers and nğ‘›nitalic_n decoder layers. The investigation experiments in Fig. 7 and Fig. 8 revealed critical insights into architectural optimization. We observed that a larger encoder is beneficial for further improving the performance as the model size increases. For the Base model in Fig. 7, the optimal configuration emerged as 8 encoder layers and 4 decoder layers, delivering superior performance and convergence speed. Notably, the Large model in Fig. 8 exhibited a distinct preference, achieving peak performance with 20 encoder layers and 4 decoder layers, an unexpectedly aggressive encoder-decoder ratio. This unexpected discovery motivates us to scale the layer ratio in DDT-XL/2 to 22 encoder layers and 6 decoders to explore the performance upper limits of diffusion transformers."
  },
  {
    "title": "Decoder Block types.",
    "level": 4,
    "content": "In our investigation of decoder block types and their impact on high-frequency decoding performance, we systematically evaluated multiple architectural configurations. Our comprehensive assessment included alternative approaches such as simple 3Ã—3 convolution blocks and naive MLP blocks. As shown in Tab. 5, the default (Attention with the MLP) setting achieves better results. Thanks to the encoder-decoder design, naive Conv blocks even achieve comparable results. DecoderBlock FIDâ†“â†“\\downarrowâ†“ sFIDâ†“â†“\\downarrowâ†“ ISâ†‘â†‘\\uparrowâ†‘ Prec.â†‘â†‘\\uparrowâ†‘ Rec.â†‘â†‘\\uparrowâ†‘ Conv+MLP 16.96 7.33 85.1 0.62 0.65 MLP+MLP 24.13 7.89 65.0 0.57 0.65 Attn+MLP 16.32 6.63 86.0 0.62 0.66 Table 5: Metrics of 400â¢K400ğ¾400K400 italic_K training steps on DDT-B/2(8En4De) with different decoder blocks. All results are reported without classifier-free guidance. The Default Attention + MLP configuration achieves best performance."
  },
  {
    "title": "6 Conclusion",
    "level": 2,
    "content": "In this paper, we have introduced a novel Decoupled Diffusion Transformer, which rethinks the optimization dilemma of the traditional diffusion transformer. By decoupling the low-frequency encoding and high-frequency decoding into dedicated components, we effectively resolved the optimization dilemma that has constrained diffusion transformer. Furthermore, we discovered that increasing the encoder capacity relative to the decoder yields increasingly beneficial results as the overall model scale grows. This insight provides valuable guidance for future model scaling efforts. Our experiments demonstrate that our DDT-XL/2 (22En6De) with an unexpected aggressive encoder-decoder layer ratio achieves great performance while requiring only 256 training epochs. This significant improvement in efficiency addresses one of the primary limitations of diffusion models: their lengthy training requirements. The decoupled architecture also presents opportunities for inference optimization through our proposed encoder result sharing mechanism. Our statistical dynamic programming approach for determining optimal sharing strategies enables faster inference while minimizing quality degradation, demonstrating that architectural innovations can yield benefits beyond their primary design objectives."
  },
  {
    "title": "References",
    "level": 2,
    "content": "Agarwal et al. [2025] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Bao et al. [2023] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669â€“22679, 2023. Brock et al. [2018] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213â€“229. Springer, 2020. Chang et al. [2022] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315â€“11325, 2022. Chen et al. [2023] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-\\\\\\backslash\\alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen et al. [2024] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\\\\\\backslash\\sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. Chu et al. [2024] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: A unified llama interface for vision tasks. arXiv preprint arXiv:2403.00522, 2024. Deng et al. [2024] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780â€“8794, 2021. Dieleman [2024] Sander Dieleman. Diffusion is spectral autoregression, 2024. Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Fei et al. [2024] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. Gao et al. [2023a] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23164â€“23173, 2023a. Gao et al. [2023b] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164â€“23173, 2023b. Hang et al. [2023] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7441â€“7451, 2023. He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000â€“16009, 2022. Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840â€“6851, 2020. Hong et al. [2022] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565â€“26577, 2022. Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015â€“4026, 2023. Kong et al. [2024] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. KynkÃ¤Ã¤nniemi et al. [2019] Tuomas KynkÃ¤Ã¤nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. KynkÃ¤Ã¤nniemi et al. [2024] Tuomas KynkÃ¤Ã¤nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in a limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Li et al. [2024] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: A self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441â€“125468, 2024. Li et al. [2025] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:56424â€“56445, 2025. Lipman et al. [2022] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu et al. [2022] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Lu et al. [2024] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024. Ma et al. [2024a] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024a. Ma et al. [2024b] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15762â€“15772, 2024b. Nash et al. [2021] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. Oquab et al. [2023] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195â€“4205, 2023. Rissanen et al. [2022] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022. Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684â€“10695, 2022. Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sauer et al. [2022] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1â€“10, 2022. Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Su et al. [2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sun et al. [2024] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Teng et al. [2023] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Teng et al. [2024] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Wang et al. [2024] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcn-like architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024. Yan et al. [2023] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. Yao and Wang [2025] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Yu et al. [2024a] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024a. Yu et al. [2024b] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. Yue et al. [2024] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. Zhuo et al. [2024] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024."
  },
  {
    "title": "Appendix A Model Specs",
    "level": 2,
    "content": "Config #Layers Hidden dim #Heads B/2 12 768 12 L/2 24 1024 16 XL/2 28 1152 16"
  },
  {
    "title": "Appendix B Hyper-parameters",
    "level": 2,
    "content": "VAE SD-VAE-f8d4-ft-ema VAE donwsample 8 latent channel 4 optimizer AdamW [22] base learning rate 1e-4 weight decay 0.0 batch size 256 learning rate schedule constant augmentation center crop diffusion sampler Euler-ODE diffusion steps 250 evaluation suite ADM [10]"
  },
  {
    "title": "Appendix C Linear flow and Diffusion",
    "level": 2,
    "content": "Given the SDE forward and reverse process: dâ¢ğ’™tğ‘‘subscriptğ’™ğ‘¡\\displaystyle{d}{\\boldsymbol{x}}_{t}italic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =fâ¢(t)â¢ğ’™tâ¢dâ¢t+gâ¢(t)â¢dâ¢ğ’˜absentğ‘“ğ‘¡subscriptğ’™ğ‘¡dğ‘¡ğ‘”ğ‘¡dğ’˜\\displaystyle=f(t){\\boldsymbol{x}}_{t}\\mathrm{d}t+g(t)\\mathrm{d}{\\boldsymbol{w}}= italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_d italic_t + italic_g ( italic_t ) roman_d bold_italic_w (9) dâ¢ğ’™tğ‘‘subscriptğ’™ğ‘¡\\displaystyle{d}{\\boldsymbol{x}}_{t}italic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =[fâ¢(t)â¢ğ’™tâˆ’gâ¢(t)2â¢âˆ‡ğ’™logâ¡pâ¢(ğ’™t)]â¢dâ¢t+gâ¢(t)â¢dâ¢ğ’˜absentdelimited-[]ğ‘“ğ‘¡subscriptğ’™ğ‘¡ğ‘”superscriptğ‘¡2subscriptâˆ‡ğ’™ğ‘subscriptğ’™ğ‘¡ğ‘‘ğ‘¡ğ‘”ğ‘¡ğ‘‘ğ’˜\\displaystyle=[f(t){\\boldsymbol{x}}_{t}-g(t)^{2}\\nabla_{\\boldsymbol{x}}\\log p(% {\\boldsymbol{x}}_{t})]dt+g(t){d}{\\boldsymbol{w}}= [ italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] italic_d italic_t + italic_g ( italic_t ) italic_d bold_italic_w (10) A corresponding deterministic process exists with trajectories sharing the same marginal probability densities of reverse SDE. dâ¢ğ’™t=[fâ¢(t)â¢ğ’™tâˆ’12â¢gâ¢(t)2â¢âˆ‡ğ’™tlogâ¡pâ¢(ğ’™t)]â¢dâ¢tğ‘‘subscriptğ’™ğ‘¡delimited-[]ğ‘“ğ‘¡subscriptğ’™ğ‘¡12ğ‘”superscriptğ‘¡2subscriptâˆ‡subscriptğ’™ğ‘¡ğ‘subscriptğ’™ğ‘¡ğ‘‘ğ‘¡{d}{\\boldsymbol{x}}_{t}=[f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{% \\boldsymbol{x}_{t}}\\log p({\\boldsymbol{x}}_{t})]{d}titalic_d bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = [ italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] italic_d italic_t (11) Given xt=Î±tâ¢xdâ¢aâ¢tâ¢a+Ïƒâ¢Ïµsubscriptğ‘¥ğ‘¡subscriptğ›¼ğ‘¡subscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘ğœitalic-Ïµx_{t}=\\alpha_{t}x_{data}+\\sigma\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_Ïƒ italic_Ïµ. The traditional diffusion model learns: âˆ‡ğ’™tlogâ¡pâ¢(ğ’™t)=âˆ’ÏµÏƒâ¢(t)subscriptâˆ‡subscriptğ’™ğ‘¡ğ‘subscriptğ’™ğ‘¡italic-Ïµğœğ‘¡\\nabla_{\\boldsymbol{x}_{t}}\\log p({\\boldsymbol{x}}_{t})=-{\\frac{\\epsilon}{% \\sigma(t)}}âˆ‡ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - divide start_ARG italic_Ïµ end_ARG start_ARG italic_Ïƒ ( italic_t ) end_ARG (12) The flow-matching framework actually learns the following: ğ’—tsubscriptğ’—ğ‘¡\\displaystyle{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT =Î±Ë™â¢x+ÏƒË™â¢ÏµabsentË™ğ›¼ğ‘¥Ë™ğœitalic-Ïµ\\displaystyle=\\dot{\\alpha}x+\\dot{\\sigma}\\epsilon= overË™ start_ARG italic_Î± end_ARG italic_x + overË™ start_ARG italic_Ïƒ end_ARG italic_Ïµ (13) =xâˆ’Ïµabsentğ‘¥italic-Ïµ\\displaystyle=x-\\epsilon= italic_x - italic_Ïµ (14) Here we will demonstrate in flow-matching, the ğ’—tsubscriptğ’—ğ‘¡{\\boldsymbol{v}}_{t}bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT prediction is actually as same as the reverse ode: Î±Ë™â¢x+ÏƒË™â¢ÏµË™ğ›¼ğ‘¥Ë™ğœitalic-Ïµ\\displaystyle\\dot{\\alpha}x+\\dot{\\sigma}\\epsilonoverË™ start_ARG italic_Î± end_ARG italic_x + overË™ start_ARG italic_Ïƒ end_ARG italic_Ïµ (15) =\\displaystyle== fâ¢(t)â¢ğ’™tâˆ’12â¢gâ¢(t)2â¢âˆ‡ğ’™tlogâ¡pâ¢(ğ’™t)ğ‘“ğ‘¡subscriptğ’™ğ‘¡12ğ‘”superscriptğ‘¡2subscriptâˆ‡subscriptğ’™ğ‘¡ğ‘subscriptğ’™ğ‘¡\\displaystyle f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{\\boldsymbol{% x}_{t}}\\log p({\\boldsymbol{x}}_{t})italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (16) Let us start by expanding the reverse ode first. fâ¢(t)â¢ğ’™tâˆ’12â¢gâ¢(t)2â¢âˆ‡ğ’™tlogâ¡pâ¢(ğ’™t)ğ‘“ğ‘¡subscriptğ’™ğ‘¡12ğ‘”superscriptğ‘¡2subscriptâˆ‡subscriptğ’™ğ‘¡ğ‘subscriptğ’™ğ‘¡\\displaystyle f(t){\\boldsymbol{x}}_{t}-\\frac{1}{2}g(t)^{2}\\nabla_{\\boldsymbol{% x}_{t}}\\log p({\\boldsymbol{x}}_{t})italic_f ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (17) =\\displaystyle== fâ¢(t)â¢(Î±â¢(t)â¢ğ’™dâ¢aâ¢tâ¢a+Ïƒâ¢(t)â¢Ïµ)âˆ’12â¢gâ¢(t)2â¢[âˆ’ÏµÏƒâ¢(t)]ğ‘“ğ‘¡ğ›¼ğ‘¡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘ğœğ‘¡italic-Ïµ12ğ‘”superscriptğ‘¡2delimited-[]italic-Ïµğœğ‘¡\\displaystyle f(t)(\\alpha(t){\\boldsymbol{x}}_{data}+\\sigma(t)\\epsilon)-\\frac{1% }{2}g(t)^{2}[{-\\frac{\\epsilon}{\\sigma(t)}}]italic_f ( italic_t ) ( italic_Î± ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_Ïƒ ( italic_t ) italic_Ïµ ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ - divide start_ARG italic_Ïµ end_ARG start_ARG italic_Ïƒ ( italic_t ) end_ARG ] (18) =\\displaystyle== fâ¢(t)â¢Î±â¢(t)â¢ğ’™dâ¢aâ¢tâ¢a+(fâ¢(t)â¢Ïƒâ¢(t)+12â¢gâ¢(t)2Ïƒâ¢(t))â¢Ïµğ‘“ğ‘¡ğ›¼ğ‘¡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘ğ‘“ğ‘¡ğœğ‘¡12ğ‘”superscriptğ‘¡2ğœğ‘¡italic-Ïµ\\displaystyle f(t)\\alpha(t){\\boldsymbol{x}}_{data}+(f(t)\\sigma(t)+\\frac{1}{2}{% \\frac{g(t)^{2}}{\\sigma(t)}})\\epsilonitalic_f ( italic_t ) italic_Î± ( italic_t ) bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + ( italic_f ( italic_t ) italic_Ïƒ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Ïƒ ( italic_t ) end_ARG ) italic_Ïµ (19) To prove Eq. 16, we needs to demonstrate that: Î±Ë™â¢(t)Ë™ğ›¼ğ‘¡\\displaystyle\\dot{\\alpha}(t)overË™ start_ARG italic_Î± end_ARG ( italic_t ) =ftâ¢Î±â¢(t)absentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡\\displaystyle=f_{t}\\alpha(t)= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) (20) ÏƒË™â¢(t)Ë™ğœğ‘¡\\displaystyle\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) =ftâ¢Ïƒâ¢(t)+12â¢gt2Ïƒâ¢(t).absentsubscriptğ‘“ğ‘¡ğœğ‘¡12superscriptsubscriptğ‘”ğ‘¡2ğœğ‘¡\\displaystyle=f_{t}\\sigma(t)+\\frac{1}{2}\\frac{{g_{t}^{2}}}{\\sigma(t)}.= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïƒ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Ïƒ ( italic_t ) end_ARG . (21) Here, let us derive the relation between ftsubscriptğ‘“ğ‘¡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î±â¢(t),Î±Ë™â¢(t)ğ›¼ğ‘¡Ë™ğ›¼ğ‘¡\\alpha(t),\\dot{\\alpha}(t)italic_Î± ( italic_t ) , overË™ start_ARG italic_Î± end_ARG ( italic_t ). We donate xdâ¢aâ¢tâ¢aâ¢(t)=Î±â¢(t)â¢xdâ¢aâ¢tâ¢asubscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡ğ›¼ğ‘¡subscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘x_{data}(t)=\\alpha(t)x_{data}italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) = italic_Î± ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT is the remain component of xdâ¢aâ¢tâ¢asubscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘x_{data}italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT in xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, it is easy to find that: dâ¢ğ’™dâ¢aâ¢tâ¢aâ¢(t)ğ‘‘subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡\\displaystyle d{\\boldsymbol{x}}_{data}(t)italic_d bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) =ftâ¢ğ’™dâ¢aâ¢tâ¢aâ¢(t)â¢dâ¢tabsentsubscriptğ‘“ğ‘¡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡ğ‘‘ğ‘¡\\displaystyle=f_{t}{\\boldsymbol{x}}_{data}(t)dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_t ) italic_d italic_t (22) dâ¢(Î±â¢(t)â¢xdâ¢aâ¢tâ¢a)ğ‘‘ğ›¼ğ‘¡subscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘\\displaystyle d(\\alpha(t)x_{data})italic_d ( italic_Î± ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ) =ftâ¢Î±â¢(t)â¢xdâ¢aâ¢tâ¢aâ¢dâ¢tabsentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡subscriptğ‘¥ğ‘‘ğ‘ğ‘¡ğ‘ğ‘‘ğ‘¡\\displaystyle=f_{t}\\alpha(t)x_{data}dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT italic_d italic_t (23) dâ¢Î±â¢(t)ğ‘‘ğ›¼ğ‘¡\\displaystyle d\\alpha(t)italic_d italic_Î± ( italic_t ) =ftâ¢Î±â¢(t)â¢dâ¢tabsentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡ğ‘‘ğ‘¡\\displaystyle=f_{t}\\alpha(t)dt= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) italic_d italic_t (24) So, Eq. 20 is right. Based on the above equation, we will demonstrate the relation of gt,ftsubscriptğ‘”ğ‘¡subscriptğ‘“ğ‘¡g_{t},f_{t}italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with Ïƒâ¢(t)ğœğ‘¡\\sigma(t)italic_Ïƒ ( italic_t ). Note that Gaussian noise has nice additive properties. aâ¢Ïµ1+bâ¢Ïµ2âˆˆğ’©â¢(0,a2+b2)ğ‘subscriptitalic-Ïµ1ğ‘subscriptitalic-Ïµ2ğ’©0superscriptğ‘2superscriptğ‘2a\\epsilon_{1}+b\\epsilon_{2}\\in\\mathcal{N}(0,\\sqrt{a^{2}+b^{2}})italic_a italic_Ïµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b italic_Ïµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT âˆˆ caligraphic_N ( 0 , square-root start_ARG italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_b start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) (25) Let us start with the gaussian noise component Ïµâ¢(t)italic-Ïµğ‘¡\\epsilon(t)italic_Ïµ ( italic_t ) calculation, reaching at tğ‘¡titalic_t, every noise addition at sâˆˆ[0,t]ğ‘ 0ğ‘¡s\\in[0,t]italic_s âˆˆ [ 0 , italic_t ] while been decayed by a factor of Î±â¢(t)Î±â¢(s)ğ›¼ğ‘¡ğ›¼ğ‘ \\frac{\\alpha(t)}{\\alpha(s)}divide start_ARG italic_Î± ( italic_t ) end_ARG start_ARG italic_Î± ( italic_s ) end_ARG. Thus, the mixed Gaussian noise will have a std variance Ïƒâ¢(t)ğœğ‘¡\\sigma(t)italic_Ïƒ ( italic_t ) of: Ïƒâ¢(t)ğœğ‘¡\\displaystyle\\sigma(t)italic_Ïƒ ( italic_t ) =(âˆ«0t[(Î±â¢(t)Î±â¢(s))2â¢gs2]â¢ğ‘‘s)absentsuperscriptsubscript0ğ‘¡delimited-[]superscriptğ›¼ğ‘¡ğ›¼ğ‘ 2subscriptsuperscriptğ‘”2ğ‘ differential-dğ‘ \\displaystyle=\\sqrt{(\\int_{0}^{t}[(\\frac{\\alpha(t)}{\\alpha(s)})^{2}g^{2}_{s}]% ds)}= square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_Î± ( italic_t ) end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ] italic_d italic_s ) end_ARG (26) Ïƒâ¢(t)ğœğ‘¡\\displaystyle\\sigma(t)italic_Ïƒ ( italic_t ) =Î±â¢(t)â¢(âˆ«0t[(gsÎ±â¢(s))2]â¢ğ‘‘s)absentğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘ ğ›¼ğ‘ 2differential-dğ‘ \\displaystyle=\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{s}}{\\alpha(s)})^{2}]ds)}= italic_Î± ( italic_t ) square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG (27) After obtaining the relation of ft,gtsubscriptğ‘“ğ‘¡subscriptğ‘”ğ‘¡{f_{t},g_{t}}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î±â¢(t),Ïƒâ¢(t)ğ›¼ğ‘¡ğœğ‘¡{\\alpha(t),\\sigma(t)}italic_Î± ( italic_t ) , italic_Ïƒ ( italic_t ), we derive Î±Ë™â¢(t)Ë™ğ›¼ğ‘¡\\dot{\\alpha}(t)overË™ start_ARG italic_Î± end_ARG ( italic_t ) and ÏƒË™â¢(t)Ë™ğœğ‘¡\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) with above conditions: Î±Ë™â¢(t)Ë™ğ›¼ğ‘¡\\displaystyle\\dot{\\alpha}(t)overË™ start_ARG italic_Î± end_ARG ( italic_t ) =ftâ¢expâ¡[âˆ«0tfsâ¢ğ‘‘s]absentsubscriptğ‘“ğ‘¡subscriptsuperscriptğ‘¡0subscriptğ‘“ğ‘ differential-dğ‘ \\displaystyle=f_{t}\\exp[\\int^{t}_{0}f_{s}ds]= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_exp [ âˆ« start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_d italic_s ] (28) Î±Ë™â¢(t)Ë™ğ›¼ğ‘¡\\displaystyle\\dot{\\alpha}(t)overË™ start_ARG italic_Î± end_ARG ( italic_t ) =ftâ¢Î±â¢(t)absentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡\\displaystyle=f_{t}\\alpha(t)= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) (29) As for ÏƒË™â¢(t)Ë™ğœğ‘¡\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ), it is quit complex but not hard: ÏƒË™â¢(t)Ë™ğœğ‘¡\\displaystyle\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) =Î±Ë™â¢(t)â¢(âˆ«0t[(gtÎ±â¢(s))2]â¢ğ‘‘s)+Î±â¢(t)â¢12â¢gt2Î±â¢(t)(âˆ«0t[(gtÎ±â¢(s))2â¢gs2]â¢ğ‘‘s)absentË™ğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2differential-dğ‘ ğ›¼ğ‘¡12superscriptsubscriptğ‘”ğ‘¡2ğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2subscriptsuperscriptğ‘”2ğ‘ differential-dğ‘ \\displaystyle=\\dot{\\alpha}(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}% ]ds)}+\\alpha(t)\\frac{\\frac{1}{2}\\frac{g_{t}^{2}}{\\alpha(t)}}{\\sqrt{(\\int_{0}^{% t}[(\\frac{g_{t}}{\\alpha(s)})^{2}g^{2}_{s}]ds)}}= overË™ start_ARG italic_Î± end_ARG ( italic_t ) square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + italic_Î± ( italic_t ) divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± ( italic_t ) end_ARG end_ARG start_ARG square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ] italic_d italic_s ) end_ARG end_ARG (30) ÏƒË™â¢(t)Ë™ğœğ‘¡\\displaystyle\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) =(ftâ¢Î±â¢(t))â¢(âˆ«0t[(gtÎ±â¢(s))2]â¢ğ‘‘s)+Î±â¢(t)â¢12â¢gt2Î±2â¢(t)(âˆ«0t[(gtÎ±â¢(s))2]â¢ğ‘‘s)absentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2differential-dğ‘ ğ›¼ğ‘¡12subscriptsuperscriptğ‘”2ğ‘¡superscriptğ›¼2ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2differential-dğ‘ \\displaystyle=(f_{t}\\alpha(t))\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2% }]ds)}+\\alpha(t)\\frac{\\frac{1}{2}\\frac{g^{2}_{t}}{\\alpha^{2}(t)}}{\\sqrt{(\\int_% {0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}]ds)}}= ( italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) ) square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + italic_Î± ( italic_t ) divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_t ) end_ARG end_ARG start_ARG square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG end_ARG (31) ÏƒË™â¢(t)Ë™ğœğ‘¡\\displaystyle\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) =ftâ¢Î±â¢(t)â¢(âˆ«0t[(gtÎ±â¢(s))2]â¢ğ‘‘s)+12â¢gt2Î±â¢(t)â¢(âˆ«0t[(gtÎ±â¢(s))2]â¢ğ‘‘s)absentsubscriptğ‘“ğ‘¡ğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2differential-dğ‘ 12superscriptsubscriptğ‘”ğ‘¡2ğ›¼ğ‘¡superscriptsubscript0ğ‘¡delimited-[]superscriptsubscriptğ‘”ğ‘¡ğ›¼ğ‘ 2differential-dğ‘ \\displaystyle=f_{t}\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{\\alpha(s)})^{2}]% ds)}+\\frac{\\frac{1}{2}g_{t}^{2}}{\\alpha(t)\\sqrt{(\\int_{0}^{t}[(\\frac{g_{t}}{% \\alpha(s)})^{2}]ds)}}= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î± ( italic_t ) square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG + divide start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± ( italic_t ) square-root start_ARG ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ ( divide start_ARG italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_Î± ( italic_s ) end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] italic_d italic_s ) end_ARG end_ARG (32) ÏƒË™â¢(t)Ë™ğœğ‘¡\\displaystyle\\dot{\\sigma}(t)overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) =ftâ¢Ïƒâ¢(t)+12â¢gâ¢tÏƒâ¢(t)absentsubscriptğ‘“ğ‘¡ğœğ‘¡12ğ‘”ğ‘¡ğœğ‘¡\\displaystyle=f_{t}\\sigma(t)+\\frac{1}{2}\\frac{gt}{\\sigma(t)}= italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïƒ ( italic_t ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG italic_g italic_t end_ARG start_ARG italic_Ïƒ ( italic_t ) end_ARG (33) So, Eq. 21 is right."
  },
  {
    "title": "Appendix D Proof of Spectrum Autoregressive",
    "level": 2,
    "content": "Given the noise scheduler{Î±t,Ïƒt}subscriptğ›¼ğ‘¡subscriptğœğ‘¡\\{\\alpha_{t},\\sigma_{t}\\}{ italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }, the clean data ğ’™datasubscriptğ’™data{\\boldsymbol{x}}_{\\text{data}}bold_italic_x start_POSTSUBSCRIPT data end_POSTSUBSCRIPT and Gaussian noise Ïµitalic-Ïµ\\epsilonitalic_Ïµ. Denote Kfâ¢râ¢eâ¢qsubscriptğ¾ğ‘“ğ‘Ÿğ‘’ğ‘K_{freq}italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT as the maximum frequency of the clean data ğ’™dâ¢aâ¢tâ¢asubscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘{\\boldsymbol{x}}_{data}bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT The noisy latent xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestep tğ‘¡titalic_t has been defined as: ğ’™t=Î±tâ¢ğ’™dâ¢aâ¢tâ¢a+Ïƒtâ¢Ïµsubscriptğ’™ğ‘¡subscriptğ›¼ğ‘¡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘subscriptğœğ‘¡bold-italic-Ïµ{\\boldsymbol{x}}_{t}=\\alpha_{t}{\\boldsymbol{x}}_{data}+\\sigma_{t}{\\boldsymbol{% \\epsilon}}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_Ïµ (34) The spectrum magnitude ğ’„isubscriptğ’„ğ‘–{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTof xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT on DCT basics ğ’–isubscriptğ’–ğ‘–{\\boldsymbol{u}}_{i}bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT follows: ğ’„isubscriptğ’„ğ‘–\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =ğ”¼Ïµâ¢[ğ’–iTâ¢ğ’™t]2absentsubscriptğ”¼italic-Ïµsuperscriptdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘¡2\\displaystyle=\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{% t}]^{2}= blackboard_E start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ğ’„isubscriptğ’„ğ‘–\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =ğ”¼Ïµâ¢[ğ’–iTâ¢(Î±tâ¢ğ’™dâ¢aâ¢tâ¢a+Ïƒtâ¢Ïµ)]2absentsubscriptğ”¼italic-Ïµsuperscriptdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ›¼ğ‘¡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘subscriptğœğ‘¡bold-italic-Ïµ2\\displaystyle=\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}(\\alpha_{t}{% \\boldsymbol{x}}_{data}+\\sigma_{t}{\\boldsymbol{\\epsilon}})]^{2}= blackboard_E start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT + italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_Ïµ ) ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Recall that the spectrum magnitude of Gaussian noise Ïµitalic-Ïµ\\epsilonitalic_Ïµ is uniformly distributed. ğ’„isubscriptğ’„ğ‘–\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =[Î±tâ¢ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢a]2+2â¢Î±tâ¢Ïƒtâ¢ğ”¼Ïµâ¢[ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢aâ¢ğ’–iTâ¢Ïµ]+Ïƒt2â¢ğ”¼Ïµâ¢[ğ’–iTâ¢Ïµ]2absentsuperscriptdelimited-[]subscriptğ›¼ğ‘¡superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘22subscriptğ›¼ğ‘¡subscriptğœğ‘¡subscriptğ”¼italic-Ïµdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘superscriptsubscriptğ’–ğ‘–ğ‘‡italic-Ïµsuperscriptsubscriptğœğ‘¡2subscriptğ”¼bold-italic-Ïµsuperscriptdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡italic-Ïµ2\\displaystyle=[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}+% 2\\alpha_{t}\\sigma_{t}\\mathbb{E}_{\\epsilon}[{\\boldsymbol{u}}_{i}^{T}{% \\boldsymbol{x}}_{data}{\\boldsymbol{u}}_{i}^{T}\\epsilon]+\\sigma_{t}^{2}\\mathbb{% E}_{\\boldsymbol{\\epsilon}}[{\\boldsymbol{u}}_{i}^{T}\\epsilon]^{2}= [ italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Ïµ ] + italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_Ïµ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Ïµ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ğ’„isubscriptğ’„ğ‘–\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =[Î±tâ¢ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢a]2+Ïƒt2â¢ğ”¼Ïµâ¢[ğ’–iTâ¢Ïµ]2absentsuperscriptdelimited-[]subscriptğ›¼ğ‘¡superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘2superscriptsubscriptğœğ‘¡2subscriptğ”¼bold-italic-Ïµsuperscriptdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡italic-Ïµ2\\displaystyle=[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}+% \\sigma_{t}^{2}\\mathbb{E}_{\\boldsymbol{\\epsilon}}[{\\boldsymbol{u}}_{i}^{T}% \\epsilon]^{2}= [ italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_Ïµ end_POSTSUBSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Ïµ ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ğ’„isubscriptğ’„ğ‘–\\displaystyle{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =Î±t2â¢[ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢a]2+Ïƒt2â¢Î»absentsuperscriptsubscriptğ›¼ğ‘¡2superscriptdelimited-[]superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘2superscriptsubscriptğœğ‘¡2ğœ†\\displaystyle=\\alpha_{t}^{2}[{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^% {2}+\\sigma_{t}^{2}\\lambda= italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Î» if Ïƒt2â¢Î»superscriptsubscriptğœğ‘¡2ğœ†\\sigma_{t}^{2}\\lambdaitalic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Î» has bigger value than [Î±tâ¢ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢a]2superscriptdelimited-[]subscriptğ›¼ğ‘¡superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘2[\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}]^{2}[ italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, the spectrum magnitude ğ’„isubscriptğ’„ğ‘–{\\boldsymbol{c}}_{i}bold_italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT on DCT basics ğ’–isubscriptğ’–ğ‘–{\\boldsymbol{u}}_{i}bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT will be canceled, thus the maximal remaining frequency fmâ¢aâ¢xâ¢(t)subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡f_{max}(t)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) of original data in ğ’™tsubscriptğ’™ğ‘¡{\\boldsymbol{x}}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows: fmâ¢aâ¢xâ¢(t)>minâ¡((Î±tâ¢ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢aÏƒtâ¢Î»)2,Kfâ¢râ¢eâ¢q)subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡superscriptsubscriptğ›¼ğ‘¡superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘subscriptğœğ‘¡ğœ†2subscriptğ¾ğ‘“ğ‘Ÿğ‘’ğ‘f_{max}(t)>\\min\\left({\\left(\\frac{\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{% \\boldsymbol{x}}_{data}}{\\sigma_{t}\\lambda}\\right)}^{2},K_{freq}\\right)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT end_ARG start_ARG italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î» end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) (35) Though Î±tâ¢ğ’–iTâ¢ğ’™dâ¢aâ¢tâ¢aÏƒtâ¢Î»2superscriptsubscriptğ›¼ğ‘¡superscriptsubscriptğ’–ğ‘–ğ‘‡subscriptğ’™ğ‘‘ğ‘ğ‘¡ğ‘subscriptğœğ‘¡ğœ†2{\\frac{\\alpha_{t}{\\boldsymbol{u}}_{i}^{T}{\\boldsymbol{x}}_{data}}{\\sigma_{t}% \\lambda}}^{2}divide start_ARG italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT end_ARG start_ARG italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Î» end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT depends on the dataset. Here, we directly suppose it as a constant 1111. And replace Î±=tğ›¼ğ‘¡\\alpha=titalic_Î± = italic_t and Ïƒ=1âˆ’tğœ1ğ‘¡\\sigma=1-titalic_Ïƒ = 1 - italic_t in above equation: fmâ¢aâ¢xâ¢(t)>minâ¡((t1âˆ’t)2,Kfâ¢râ¢eâ¢q)subscriptğ‘“ğ‘šğ‘ğ‘¥ğ‘¡superscriptğ‘¡1ğ‘¡2subscriptğ¾ğ‘“ğ‘Ÿğ‘’ğ‘f_{max}(t)>\\min\\left({\\left(\\frac{t}{1-t}\\right)}^{2},K_{freq}\\right)italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( italic_t ) > roman_min ( ( divide start_ARG italic_t end_ARG start_ARG 1 - italic_t end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_K start_POSTSUBSCRIPT italic_f italic_r italic_e italic_q end_POSTSUBSCRIPT ) (36)"
  },
  {
    "title": "Appendix E Linear multisteps method",
    "level": 2,
    "content": "We conduct targeted experiment on SiT-XL/2 with Adamsâ€“Bashforth like linear multistep solver; To clarify, we did not employ this powerful solver for our DDT models in all tables across the main paper. The reverse ode of the diffusion models tackles the following integral: ğ’™i+1=ğ’™i+âˆ«titi+1ğ’—Î¸â¢(ğ’™t,t)â¢ğ‘‘tsubscriptğ’™ğ‘–1subscriptğ’™ğ‘–superscriptsubscriptsubscriptğ‘¡ğ‘–subscriptğ‘¡ğ‘–1subscriptğ’—ğœƒsubscriptğ’™ğ‘¡ğ‘¡differential-dğ‘¡{\\boldsymbol{x}}_{i+1}={\\boldsymbol{x}}_{i}+\\int_{t_{i}}^{t_{i+1}}{\\boldsymbol% {v}}_{\\theta}({\\boldsymbol{x}}_{t},t){dt}\\\\ bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + âˆ« start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) italic_d italic_t (37) The classic Euler method employs ğ’—Î¸â¢(ğ’™i,ti)subscriptğ’—ğœƒsubscriptğ’™ğ‘–subscriptğ‘¡ğ‘–{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{i},t_{i})bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) as an estimate of ğ’—Î¸â¢(ğ’™t,t)subscriptğ’—ğœƒsubscriptğ’™ğ‘¡ğ‘¡{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{t},t)bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) throughout the interval [ti,ti+1]subscriptğ‘¡ğ‘–subscriptğ‘¡ğ‘–1[t_{i},t_{i+1}][ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ] ğ’™i+1=ğ’™i+(ti+1âˆ’ti)â¢ğ’—Î¸â¢(ğ’™i,ti).subscriptğ’™ğ‘–1subscriptğ’™ğ‘–subscriptğ‘¡ğ‘–1subscriptğ‘¡ğ‘–subscriptğ’—ğœƒsubscriptğ’™ğ‘–subscriptğ‘¡ğ‘–{\\boldsymbol{x}}_{i+1}={\\boldsymbol{x}}_{i}+(t_{i+1}-t_{i}){\\boldsymbol{v}}_{% \\theta}({\\boldsymbol{x}}_{i},t_{i}).\\\\ bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + ( italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . (38) The most classic multi-step solver Adamsâ€“Bashforth method (deemed as Adams for brevity) incorporates the Lagrange polynomial to improve the estimation accuracy with previous predictions. ğ’—Î¸â¢(ğ’™t,t)subscriptğ’—ğœƒsubscriptğ’™ğ‘¡ğ‘¡\\displaystyle{\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{t},t)bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) =âˆ‘j=0i(âˆk=0,kâ‰ jitâˆ’tktjâˆ’tk)â¢ğ’—Î¸â¢(ğ’™j,tj)absentsuperscriptsubscriptğ‘—0ğ‘–superscriptsubscriptproductformulae-sequenceğ‘˜0ğ‘˜ğ‘—ğ‘–ğ‘¡subscriptğ‘¡ğ‘˜subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘˜subscriptğ’—ğœƒsubscriptğ’™ğ‘—subscriptğ‘¡ğ‘—\\displaystyle=\\sum_{j=0}^{i}(\\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k% }}}){\\boldsymbol{v}}_{\\theta}({\\boldsymbol{x}}_{j},t_{j})= âˆ‘ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( âˆ start_POSTSUBSCRIPT italic_k = 0 , italic_k â‰  italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ğ’™i+1subscriptğ’™ğ‘–1\\displaystyle{\\boldsymbol{x}}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT â‰ˆğ’™i+âˆ«titi+1âˆ‘j=0i(âˆk=0,kâ‰ jitâˆ’tktjâˆ’tk)â¢ğ’—Î¸â¢(ğ’™j,tj)â¢dâ¢tabsentsubscriptğ’™ğ‘–superscriptsubscriptsubscriptğ‘¡ğ‘–subscriptğ‘¡ğ‘–1superscriptsubscriptğ‘—0ğ‘–superscriptsubscriptproductformulae-sequenceğ‘˜0ğ‘˜ğ‘—ğ‘–ğ‘¡subscriptğ‘¡ğ‘˜subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘˜subscriptğ’—ğœƒsubscriptğ’™ğ‘—subscriptğ‘¡ğ‘—ğ‘‘ğ‘¡\\displaystyle\\approx{\\boldsymbol{x}}_{i}+\\int_{t_{i}}^{t_{i+1}}\\sum_{j=0}^{i}(% \\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k}}}){\\boldsymbol{v}}_{\\theta}% ({\\boldsymbol{x}}_{j},t_{j})dtâ‰ˆ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + âˆ« start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( âˆ start_POSTSUBSCRIPT italic_k = 0 , italic_k â‰  italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) italic_d italic_t ğ’™i+1subscriptğ’™ğ‘–1\\displaystyle{\\boldsymbol{x}}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT â‰ˆğ’™i+âˆ‘j=0iğ’—Î¸â¢(ğ’™j,tj)â¢âˆ«titi+1(âˆk=0,kâ‰ jitâˆ’tktjâˆ’tk)â¢ğ‘‘tabsentsubscriptğ’™ğ‘–superscriptsubscriptğ‘—0ğ‘–subscriptğ’—ğœƒsubscriptğ’™ğ‘—subscriptğ‘¡ğ‘—superscriptsubscriptsubscriptğ‘¡ğ‘–subscriptğ‘¡ğ‘–1superscriptsubscriptproductformulae-sequenceğ‘˜0ğ‘˜ğ‘—ğ‘–ğ‘¡subscriptğ‘¡ğ‘˜subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘˜differential-dğ‘¡\\displaystyle\\approx{\\boldsymbol{x}}_{i}+\\sum_{j=0}^{i}{\\boldsymbol{v}}_{% \\theta}({\\boldsymbol{x}}_{j},t_{j})\\int_{t_{i}}^{t_{i+1}}(\\prod_{k=0,k\\neq j}^% {i}{\\frac{t-t_{k}}{t_{j}-t_{k}}})dtâ‰ˆ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + âˆ‘ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) âˆ« start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( âˆ start_POSTSUBSCRIPT italic_k = 0 , italic_k â‰  italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) italic_d italic_t Note that âˆ«titi+1(âˆk=0,kâ‰ jitâˆ’tktjâˆ’tk)â¢ğ‘‘tsuperscriptsubscriptsubscriptğ‘¡ğ‘–subscriptğ‘¡ğ‘–1superscriptsubscriptproductformulae-sequenceğ‘˜0ğ‘˜ğ‘—ğ‘–ğ‘¡subscriptğ‘¡ğ‘˜subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘˜differential-dğ‘¡\\int_{t_{i}}^{t_{i+1}}(\\prod_{k=0,k\\neq j}^{i}{\\frac{t-t_{k}}{t_{j}-t_{k}}})dtâˆ« start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( âˆ start_POSTSUBSCRIPT italic_k = 0 , italic_k â‰  italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_t - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) italic_d italic_t of the Lagrange polynomial can be pre-integrated into a constant coefficient, resulting in only naive summation being required for ODE solving."
  },
  {
    "title": "Appendix F Classifier free guidance.",
    "level": 2,
    "content": "As classifier-free guidance significantly impacts the performance of diffusion models. Traditional classifier-free guidance improves performance at the cost of decreased diversity. Interval guidance is recently been adopted by REPA[52] and Causalfusion[9], It applies classifier-free guidance only to the high-frequency generation phase to preserve the diversity. We sweep different classifier-free guidance strength with selected intervals. Our DDT-XL/2 achieves the best performance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] with a classifer-free guidance of 2. Recall that we donate t=0ğ‘¡0t=0italic_t = 0 as the pure noise timestep while REPA[52] use t=1ğ‘¡1t=1italic_t = 1, thus this exactly correspond to the [0,0.7]00.7[0,0.7][ 0 , 0.7 ] interval in REPA[52] Figure 9: FID10K of DDT-XL/2 with different Classifer free guidance strength and guidance intervals. We sweep different classifier-free guidance strength with selected intervals. Our DDT-XL/2 achieves the best performance with interval [0.3,1]0.31[0.3,1][ 0.3 , 1 ] with a classifer-free guidance of 2."
  },
  {
    "title": "Instructions for reporting errors",
    "level": 2,
    "content": "We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below: Click the \"Report Issue\" button. Open a report feedback form via keyboard, use \"Ctrl + ?\". Make a text selection and click the \"Report Issue for Selection\" button near your cursor. You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section. Our team has already identified the following issues. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all. Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions."
  }
]